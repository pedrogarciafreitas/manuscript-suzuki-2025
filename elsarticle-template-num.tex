%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
%\documentclass[preprint,12pt,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%\documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
\documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}


\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\usepackage{diagbox}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[hidelinks, draft]{hyperref}
\usepackage{url}

\usepackage{multicol}
\usepackage{multirow}

\usepackage{dsfont}
\newcommand\dsone{\mathds{1}}

\usepackage{xcolor}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\purpole}[1]{\textcolor[rgb]{0.7,0.2,0.7}{#1}}


\usepackage[nomain, acronym, symbols]{glossaries}
\usepackage{glossaries-extra}
\glsdisablehyper
\setabbreviationstyle[acronym]{long-short}


\newacronym{AI}{AI}{Artificial Intelligence}
\newacronym{AAL}{AAL}{Additive Attention Layer}
\newacronym{BERT}{BERT}{Bidirectional Encoder Representations from Transformers}
\newacronym{DPAL}{DPAL}{Dot Product Attention Layer}
\newacronym{NSDPAL}{NSDPAL}{Non-Scaled Dot Product Attention Layer}

\newacronym{PoC}{PoC}{Proof-of-Concept}
\newacronym{GPL}{GPL}{Global Pooling Layer}
\newacronym{GPT}{GPT}{Generative Pre-training Transformer}

\newacronym[longplural={Convolutional Neural Networks}, plural=CNNs]{CNN}{CNN}{Convolutional Neural Network}
\newacronym[longplural={Interbeat Intervals}, plural=IBIs]{IBI}{IBI}{Inter-Beat Interval}
\newacronym{PPG}{PPG}{Photoplethysmography}
\newacronym{PD}{PD}{Peak Detection}
\newacronym[longplural={Multilayer Perceptrons}, plural=MLPs]{MLP}{MLP}{Multilayer Perceptron}
\newacronym{HRV}{HRV}{Heart Rate Variability}
\newacronym{ML}{ML}{Machine Learning}
\newacronym{DL}{DL}{Deep Learning}
\newacronym{NAS}{NAS}{Neural Architecture Search}
\newacronym[longplural={Electrocardiogram}, plural=ECGs]{ECG}{ECG}{Electrocardiogram}
\newacronym{SQA}{SQA}{Signal Quality Assessment}
\newacronym{SOTA}{SOTA}{state-of-the-art}
\newacronym{LSTM}{LSTM}{Long Short-Term Memory}
\newacronym{TP}{TP}{True Positive}
\newacronym{TN}{TN}{True Negative}
\newacronym{FP}{FP}{False Positive}
\newacronym{FN}{FN}{False Negative}
\newacronym{MPER}{MPER}{Metric-Parameters Efficiency Ratio}
\newacronym{RRI}{RRI}{RR Interval}
\newacronym{PPI}{PPI}{Peak-to-Peak Interval}
\newacronym{HMA}{HMA}{Health Monitoring Applications}
\newacronym[longplural={Cardiovascular Diseases}, plural=CVDs]{CVD}{CVD}{Cardiovascular Disease}
\newacronym{WHO}{WHO}{World Health Organization}
\newacronym{HR}{HR}{Heart Rate}
\newacronym{LMS}{LMS}{Least-Mean-Square}
\newacronym{SQC}{SQC}{Signal Quality Classifier}
\newacronym{SpO2}{SpO2}{Peripheral Oxygen Saturation}
\newacronym{NLP}{NLP}{Natural Language Processing}
\newacronym{AF}{AF}{Atrial Fibrillation}
\newacronym{NSR}{NSR}{Normal Sinus Rhythm}
\newacronym{DFPAL}{DFPAL}{Differential Dot-Product Attention Layer}
\newacronym{DINTAL}{DINTAL}{Differential-Integral Dot-Product Attention Layer}

\newacronym{ABP}{ABP}{arterial blood pressure}
\newacronym{ACC}{ACC}{accuracy}
\newacronym{BACC}{BACC}{balanced accuracy}
\newacronym{BPM}{BPM}{beats per minute}
\newacronym{BUTPPG}{BUTPPG}{Brno University of Technology smartphone Photoplethysmography database}
\newacronym{CA}{CA}{cardiac arrhythmia}
\newacronym{GAF}{GAF}{Gramian Angular Field}
\newacronym{GADF}{GADF}{Gramian Angular Difference Field}
\newacronym{GASF}{GASF}{Gramian Angular Summation Field}

\newacronym{LOSO}{LOSO}{Leave One Subject Out}

\newacronym{GPU}{GPU}{Graphics Processing Unit}


\newacronym{MTF}{MTF}{Markov Transition Field}
\newacronym{RP}{RP}{Recurrence Plot}
\newacronym{PMix}{PMix}{Projection Mix}
\newacronym{CV}{CV}{Computer Vision}
\newacronym{IoT}{IoT}{Internet of Things}
\newacronym{LED}{LED}{light-emitting diode}
\newacronym{SESS}{SESS}{slow ejection slope sum}
\newacronym{EDSS}{EDSS}{end diastole slope sum}
\newacronym{SQI}{SQI}{signal quality index}
\newacronym{SVM}{SVM}{support vector machine}
\newacronym{FFNN}{FFNN}{feed-forward neural network}
\newacronym{DTW}{DTW}{dynamic time warping}
\newacronym{KNN}{KNN}{k-nearest neighbors}
\newacronym{ROC AUC}{ROC AUC}{area under receiver operating characteristics curve}
\newacronym{MCC}{MCC}{Matthew's correlation coefficient}


\newacronym{ViT}{ViT}{Vision Transformer}
\newacronym{MaxViT}{MaxViT}{Multi-axis Vision Transformer}
\newacronym{SwinT}{SwinT}{Shifted Windows Transformer}
\newacronym{SwinTV2}{SwinTV2}{Shifted Windows Transformer Version~2}
\newacronym{RISEC}{RISEC}{Random Interval Spectral Ensemble Classifier}
\newacronym{TDE}{TDE}{Temporal Dictionary Ensemble}
\newacronym{WiResNet}{WiResNet}{Wide ResNet}


%% Activation functions
\newacronym{ReLU}{ReLU}{Rectified Linear Unit}
\newacronym{LeakyReLU}{LeakyReLU}{Leaky version of a Rectified Linear Unit}
\newacronym{Tanh}{Tanh}{Hyperbolic Tangent}

\newacronym{LBP}{LBP}{Local Binary Pattern}


\usepackage{lineno}



% PARAMETERS
\newcommand{\SubjectTreeSize}{0.8\textwidth}
\newcommand{\RPThreshold}{0.05}
\newcommand{\RPDelay}{10}
\newcommand{\MemoryTableWidth}{\textwidth}
\newcommand{\MemoryTableReduction}{0.7}
\newcommand{\timePlotsWidth}{0.95\textwidth}
\newcommand{\NumberOfMemoryTableColumns}{2}

\newcommand{\averagesTableWidth}{\columnwidth}

\newcommand{\MemoryTabularsInclusion}[2]{
    \foreach \index in {1, ..., #1} {
        \input{tex/tabelas/resultados/memory/#2/memory_table_\index.tex}
    }
}

\newcommand{\AveragesTableDescription}[1]{\caption{Averages and standard deviations of the folds evaluation for the #1 variants.}}
\newcommand{\MemoryTableDescription}[1]{Memory size in Mega Bytes of each #1 family model variant.}
\newcommand{\TimePlotsDescription}[1]{Inference time in milliseconds of each #1 family model variant.}

\newcommand{\AveragesTable}[2]{
    \begin{table}[h!]
        \AveragesTableDescription{#1}
        \adjustbox{width=\averagesTableWidth,center}{#2}
        \label{tab:Averages_of_#2}
    \end{table}
}

\newcommand{\IncludeMemoryTable}[2]{
    \begin{table}[h!]
    \centering
    \caption{\MemoryTableDescription{#2}}
    \scalebox{\MemoryTableReduction}{
        \MemoryTabularsInclusion{\NumberOfMemoryTableColumns}{#1}
    }
    \label{tab:Memory_of_#1}
\end{table}
}





\journal{Information Sciences}

\begin{document}

\begin{frontmatter}

\title{VisionPPG: Photoplethysmography Signal Quality Assessment Through Computer Vision Techniques}

\author[1]{Guilherme Chagas Suzuki}
\ead{guilhermecsz11@gmail.com}
\ead[URL]{http://lattes.cnpq.br/6308081045104473}


% Second author
\author[1]{Pedro Garcia Freitas\corref{cor1}}
\cortext[cor1]{Corresponding author}
\ead{pedro.freitas@unb.br}
\ead[URL]{https://pedrogarcia.gitlab.io/}



%% Author affiliation
\affiliation[1]{organization={Department of Computer Science, University of Brasília},
            addressline={CIC/EST Building, Campus Darcy Ribeiro, Asa Norte}, 
            city={Brasilia},
%          citysep={}, % Uncomment if no comma needed between city and postcode
            postcode={70910-900}, 
            state={DF},
            country={Brazil}}



%% Abstract
\begin{abstract}


\gls{PPG} is a fundamental component to enable a myriad of continuous health monitoring applications. 
These applications commonly utilize wearable devices to record signals that are useful in the individual's health condition diagnostic. \gls{PPG} is widely adopted in wearable sensors due to its small form factor, non-invasive nature, and cost-effectiveness. 
However, despite these benefits, the \gls{PPG} highly susceptible to noise caused by motion and environmental interferences. Such noise can greatly impair the quality of the signal, which compromises the performance and reliability of health monitoring. 
Given that, assessing the signal quality is essential for enabling trustable health monitoring applications. Most \gls{SQA} strategies adopted in these devices rely either on \gls{ML} models, achieving high performance levels despite the associated increases in resource demand and power consumption, or rule-based strategies that can assess signal quality in an energy-efficient way with reduced accuracy. 
In this work, we present VisionPPG, a method for assessing the quality of \gls{PPG} signals through a fusion of signal projections and \gls{CV} techniques. Specifically, the one-dimensional \gls{PPG} signal is projected onto a set of bidimensional representations based on \gls{GAF}, \gls{MTF} and \gls{RP}. These projections are then aggregated to form a tensor referred as \gls{PMix}. The \gls{PMix} is combined with several \gls{CV} to model a signal quality classifier. To validate \gls{PMix}, different \glspl{CV} models are trained and tested using the \gls{BUTPPG} database. 
The results indicate that VisionPPG outperforms state-of-the-art time series classifiers, achieving a K-Fold mean Cohen Kappa score of 0.955$\pm$0.101\%, F1 Score of 0.967$\pm$0.078, and Precision of 0.944$\pm$0.130 when the \gls{PMix} is combined with a Wide ResNet.
%The source code is available at \url{https://gitlab.com/lisa-unb/projection-based-biological-signal-processing}.
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% \includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Novel VisionPPG method combines signal projections with computer vision for PPG SQA.
\item Proposed Projection Mix (PMix) fuses GAF, MTF, and RP for better signal assessment.
\item VisionPPG with Wide ResNet outperforms state-of-the-art time series classifiers.
\item Achieved a high Cohen Kappa score of 0.955 using deep learning and signal imagery.
\end{highlights}

%% Keywords
\begin{keyword}
Intelligent Systems \sep Biomedical Engineering \sep Computer Vision \sep Photoplethysmography \sep Quality Assessment
\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
\linenumbers

\section{Introduction} \label{sec_intro}


The pursuit of enhanced clinical longevity and patient quality of life has driven significant research in medical monitoring technologies. 
One of the main research areas in that regard is medicine, which provides means of preventing and remedying accidents and diseases. 
Asymptomatic cardiovascular pathologies, such as atheroma formation, present significant clinical risks, including acute ischemic events and strokes~\cite{atheromas}.
For that reason, it is vital to seek regular medical consultation to detect diseases in their early stages. The treatability of certain conditions, such as cardiac amyloidosis, is often contingent upon early diagnosis~\cite{cardiac-amyloidosis}.
However, personalized professional care can be expensive and often lacks the real-time responsiveness required to manage acute events when a patient is outside of a hospital setting.
As a result, there is a growing demand for autonomous and continuous health monitoring systems.


In this context, a promising solution for that demand is to use wearable devices for enabling affordable continuous health monitoring applications.
Examples of wearable devices include quotidian objects, with the shape of belts, bracelets, rings, shoe soles, clothing, etc~\cite{van2024smart}.
Smartwatches are a common category of wearables capable of running multiple health-monitoring applications.
These applications are facilitated by the advent of the \gls{IoT}, which is defined as a scalable network of interconnected devices that exchange data captured by various sensors~\cite{aouedi2024survey}.
These sensors capture vital physiological indicators, including thermal fluctuations, hemodynamic parameters, and neuroelectrical activity.
These sensors are significant to capture environmental and physiological data that are valuable for healthcare, since they provide indicators of the patient's physiological state.~\cite{ECG-diagnosis}. 
%For instance, certain \gls{ECG} patterns can signal a post-ischemic state~\cite{ECG-diagnosis}.



Despite the advantages of continuous health monitoring using wearables, the extraction of physiological signals are often vulnerable to signal artifacts and environmental noise. 
\gls{PPG} signals, one of the most versatile sensed data in wearables, are influenced by various factors, including ambient light, hardware limitations, and dermal characteristics. These artifacts can impair signal integrity to the point that its use becomes unfeasible for diagnostic use.
Moreover, \gls{PPG} signals are highly susceptible to artifacts generated by motion or noise sources. For instance, wrist movements can disrupt the signal in a smartwatch \gls{PPG} sensor~\cite{ppg-1}, though the degree of distortion varies with the signal power and wavelength. These variation can cause high-amplitude distortions that not only can destroy the core information, but can also produce misleading events. 
Such distortions are unacceptable in healthcare contexts since noisy signals can lead the monitoring system to misdiagnosis, exposing healthy patient to unnecessary risk, while may leave an at-risk patient without essential care.
For these reasons, ensuring signal integrity is crucial before proceeding with further health status estimation.
This process, termed \glsfirst{SQA}, is responsible for classifying which segments of the incoming \gls{PPG} signal are of sufficient quality to ensure reliable measurements for the health applications.
\gls{SQA} is the focus of this paper, which proposes an objective method for evaluating \gls{PPG} signals.



\subsection{Problem description}
\label{sec:problem}

Historically, \gls{ECG} and \gls{PPG} are the two most prevalent methods for evaluating the cardiac activity and heart rate monitoring.
The \gls{ECG} has long been considered the clinical gold standard for monitoring heart rate and diagnosing cardiovascular conditions. 
It monitors the electrical impulses responsible for myocardial contractions through electrodes attached to the skin, usually positioned on the chest and limbs.
While \gls{ECG} is the clinical standard, its requirement for surface electrodes and stable skin contact makes it less suitable for continuous monitoring in unconstrained environments.
Conversely, \gls{PPG} offers a more convenient approach for monitoring cardiorespiratory status under these unconstrained scenarios.
\gls{PPG} employs compact optical sensors and a light source to detect variations in skin color caused by blood flow following each heartbeat. The \gls{PPG} measures the blood flow rate in tissues (e.g. wrist), influenced by the heart's pumping action, making it particularly effective for peripheral circulation monitoring, especially with wrist-worn or finger-mounted devices. Since both \gls{ECG} and \gls{PPG} gauge cardiovascular and circulatory parameters, they are interconnected, as depicted in Figure~\ref{fig:ecg_and_ppg}. The similarity in the signal periods of both methods suggests that either can be used to analyze metrics such as the \gls{IBI}. Additionally, Figure~\ref{fig:ecg_and_ppg} emphasizes the reference points often utilized to assess health indicators related to blood pressure, oxygen saturation, and more.
Consequently, \gls{PPG} emerges as a viable alternative to \gls{ECG}, particularly in unconstrained scenarios.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{ecg_ppg_signals.pdf}
    \caption{Inter-beat interval estimation using RR interval from \gls{ECG} and the corresponding peak-to-peak (PP) interval from \gls{PPG}.}
    \label{fig:ecg_and_ppg}
\end{figure} 
\begin{figure}[t!]
    \centering
    \adjustbox{width=1\columnwidth}{%
    \def\arraystretch{0}
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cc}
        Reliable (suitable) & Unreliable (distorted) \\
        \includegraphics[width=0.48\textwidth, trim={5.02em 0 0em 0}, clip]{butppg_111001.png} 
        & \includegraphics[width=0.48\textwidth, trim={4.12em 0 0em 0}, clip]{butppg_111003.png} \\
        \includegraphics[width=0.48\textwidth, trim={5.02em 0 0em 0}, clip]{butppg_111002.png} 
        & \includegraphics[width=0.48\textwidth, trim={4.12em 0 0em 0}, clip]{butppg_111004.png} \\
    \end{tabular}
    }
    \caption{Example of reliable (blue) and distorted (red) \gls{PPG} signals from the \gls{BUTPPG} database~\protect\cite{butppg}.}
    \label{fig:butppg_samples}
\end{figure}

Typically, \gls{PPG} signals are prone to degradation due to various factors, with motion artifacts being among the most significant~\cite{ppg-1}.
Excessive movement of the \gls{PPG} sensor can cause significant waveform distortion, introducing motion artifacts that compromise the accuracy of feature extraction.
Such artifacts mask important information by distorting signal morphology, thereby compromising the integrity of the physiological measurement.
Figure~\ref{fig:butppg_samples} illustrates examples of both reliable and distorted \gls{PPG} signals.
As shown in Figure~\ref{fig:butppg_samples}, the reliable signals exhibit symmetrical, well-defined, and more consistent patterns. Conversely, the distorted segments are irregular and asymmetrical, demonstrating lower consistency and increased variability between cardiac cycles.
These distorted signals may cause algorithmic misclassifications, which is unacceptable in health monitoring.
Therefore, methods for assessing \gls{PPG} signal quality are essential to prevent misinterpretation by differentiating between reliable and noisy data.


\subsection{Related Work}
\label{sec:related}


\gls{SQA} involves a wide range of problems in signal processing technologies, and it is not exclusive to the medical domain. Actually, its origins relate to the old communication systems, when researchers published their first works on the information theory     in the 1920s and 1930s. One example of such foundational publications is the work of Rice~\cite{origins-1}, in which he analyzes the statistical properties of communication device noises. Another example is the work of Shannon \cite{origins-2}, which introduces fundamental concepts in communication systems. In that millennium, studies already used the concept of \gls{SQA} \cite{origins-3, origins-4}. We can see samples of this in the 1980s, such as the work of Stehle \cite{origins-3}, which proposed an algorithm for assessing the quality of shortwave broadcast signals, trying to objectively measure the human perception of the signal message intelligibility. Some of its conclusions are useful in the \gls{SQA} in clinical contexts, such as the high degree of subjectivity in the human idea of quality. This makes labeling datasets properly fundamental to reflect this concept of quality in the proper evaluation of the developed \gls{SQA} algorithms.
    
Past the 20th century, the \gls{SQA} of physiological signals begin to become popular. Particularly, the early 2000s had several works in the subject. One of them is the work of Wang et al. \cite{2000s-1}, which proposed a \gls{ECG} \gls{SQA} method based on the difference between the areas of distinct QRS complexes. The work proposed comparing the cumulative histograms of different \gls{ECG} leads to assess its qualities. Later,  Li et al. \cite{2000s-2} suggested the combination of multiple quality indices and \gls{HR} to assess \gls{ECG}. Another work, by Deshmane et al. \cite{2000s-3}, posed the thresholding based on the Hjorth parameters~\cite{hjorth-parameters} to assess the quality of \gls{PPG} signals. Afterwards, Zhang et al. \cite{2000s-4} elaborated an \gls{ABP} signal quality assessment metric based on the \gls{EDSS} and \gls{SESS} features. Through those works, the researchers proposed quantifying the \gls{SQA} in a metric \cite{2000s-2, 2000s-3, 2000s-4}. A name that they used to refer to this metric was \gls{SQI} \cite{2000s-2, 2000s-3}.

Among the variety of physiological signals, the \gls{ECG} is prevalent in the literature. This signal has multiple applications, such as disease classification, heartbeat type detection, biometric detection and emotion recognition \cite{ecg-1}. There are plenty of works in the \gls{SQA} of ECG signals. In one of them, Naseri et al. \cite{ecg-2} proposed two features for the estimation of a classification \gls{SQI} of multi-channeled \gls{ECG}s. One feature consists of verifying if two energy-like indices, measured in decibels, are within an admissible range. The other feature result from randomly choosing a target lead, feeding a \gls{FFNN} with array of derivatives of all leads to reconstruct the targeted lead and finally comparing the original target lead to its artificial version with correlation analysis. Therefore, the \gls{ECG} is present in the \gls{SQA} literature.

Also, there are other publications on the \gls{ECG} \gls{SQA}. For instance, in 2017, Orphanidou et al. \cite{ecg-3} introduced a feature based on the extraction of the \gls{HRV} of \gls{ECG} signals. The method decomposes this new signal into wavelets with different frequency ranges and calculates the entropy of each of them, forming a feature vector. This vector feeds a \gls{SVM}, which classifies the signal as acceptable or not. Later, Shahriari et al. \cite{ecg-4} developed an image-based feature that measures the structural similarity measure between the input plot image, containing each signal channel Cartesian graph, and multiple template plot images of similar shape selected from the training dataset by using clustering analysis. One year later, Moeyersons et al. \cite{ecg-5} proposed transforming the signal using the auto correlation function and extracting simple features from it. In 2024, Huerta et al. \cite{ecg-6} generated phase space plots, such as Poincaré plots, and discretized them into a grid where each cell is the logarithm of the number of points contained in that cell. Thus, the \gls{SQA} literature for \gls{ECG} ranges multiple works.

Besides the relevance of the \gls{ECG}, the \gls{PPG} has increased in popularity as an alternative to it. In fact, its number of related articles published from 2013 to 2023 has increased by 176\% \cite{ppg-1}. However, the \gls{PPG} is also susceptible to noise, fact that led to the development of \gls{PPG} \gls{SQA} methods. A sample of this literature is the work of Li et al. \cite{ppg-2}, which poses the measurement of a \gls{SQI} through the application of the \gls{DTW} technique to measure the signal disparity to an established template. These authors then fed this \gls{SQI} and some others to a \gls{MLP} and to a self-made rule function, predicting a unique \gls{SQI}. Experiments on private annotations on the MIMIC II dataset resulted on the \gls{MLP} achieving the highest accuracy, 95.2\%. Another sample is the work of Papini et al. \cite{ppg-3}, in which they proposed a method that first segments the input signal by finding all negative local minima points. Then, the method creates a template signal by calculating the \gls{DTW} barycenter average of the signal segments. Finally, it calculates the \gls{SQI} for each beat by comparing them to the template. This method obtained above 95\% sensitivity and positive predictive values on two public datasets. Therefore, it is possible to achieve good predictive quality in the \gls{SQA} of \glspl{PPG}.


According to Such~\cite{such2007motion}, \gls{SQA} methods in biomedical signals generally fall into two categories: single-parameter and multiparameter approaches. Unlike single-parameter methods, multiparameter techniques utilize additional sensors that provide information related to the motion or the \gls{PPG} itself. Examples of these additional sensors include accelerometers~\cite{nabavi2020robust, tuauctan2015characterization} and optical source-detector pairs with peak responses beyond the red-infrared wavelength spectrum~\cite{zhang2019motion}. Some studies generate reference noise signals internally from the impaired \gls{PPG} segments, reducing the need for extra hardware~\cite{ram2011novel, raghuram2016use}. Additional sensor channels can also transmit data about the same or a similar physiological indicator that reacts differently to artifacts. Nevertheless, using measured or synthetic reference signals for detecting contaminated \gls{PPG} segments often relies on adaptive filtering. Beyond its high computational and mathematical complexity, this approach may require extended convergence times to reach an optimal solution.



In contrast to the techniques mentioned earlier, \gls{SQA} methods that do not rely on measured or synthetic reference signals (i.e. referenceless methods) may be better suited for wearable, real-time applications, since they eliminate the need for additional data collection and processing. In this context, \gls{ML} has made significant strides in this area by enabling the classification of \gls{PPG} signals into ``reliable'' or ``unreliable'' based on the extraction of distinguishing information and the recognition of complex patterns, either automatically or with minimal human involvement~\cite{janiesch2021machine}. 


One characteristic of the \gls{PPG} \gls{SQA} literature is the presence of both supervised and unsupervised machine learning approaches. Supervised learning involves learning features with the presence of labels. This allows the machine learning model to have access to more information in the learning process, but at the cost of more training computational expense. Multiple works in the \gls{SQA} literature are supervised. Per example, Mohagheghian et al. \cite{ppg-sqa-1} introduced a method to improve feature selection algorithms by ensembling feature subsets into a majority voting schema. A machine learning algorithm determines the voting threshold, such as AdaBoost, \gls{SVM}, \gls{KNN} and discriminant analysis. Among those predictors, the AdaBoost presented the best performance in terms of \gls{ROC AUC} and accuracy. Furthemore, the method achieved accuracies of 91.55\%, 92.29\% and 95.86\% on, respectively, the DeepBeat, UMMC Simband, and MIMICIII datasets. This result is competitive to other three compared methods, despite the labeled quality scores are not publicly available. As another example, Tiwari et al. \cite{ppg-sqa-2} proposed transforming the signal into a modulation spectrogram representation and, then, extracting features from it for subsequently feeding them to a machine learning model. Experiments compared the method to various \glspl{SQI} by feeding them to a logistic regressor. Tests involved three wavelengths: red, green and infrared. The results showed that the method outperformed the others \glspl{SQI} by 21.3\% \gls{BACC} for green, 21.6\% \gls{BACC} for red, and 19.0\% \gls{BACC} for infrared wavelengths. A final example is the work of Miranda et al. \cite{ppg-sqa-3}, in which the \gls{SQA} results from the application of a interval type-2 fuzzy logic system. Experiments on a private dataset lead to 77\% and 93.72\% \gls{MCC} and \gls{ACC}, respectively. However, it is observed that most of these studies originally utilize non-publicly unlabeled datasets, which compromises the reproducibility of their results.
    
On the other hand, unsupervised learning dismisses labels in the learning process. Even though this approach gives less information to the trained model, this makes the model independent of specialists guidance and its biases. As opposed to most works in the \gls{SQA} literature, some works present unsupervised methods. For example, Singha et al. \cite{ppg-sqa-4} propose extracting entropy and statistical features from the input signal and feeding them to a self-organizing map. Experiments on a private dataset resulted in 92.01\% accuracy in ternary classification. As another example, Mahmoudzadeh et al. \cite{ppg-sqa-5} proposed extracting features from the time and frequency domains and feeding them to a elliptical envelope algorithm. This method achieved 97\% and 93\% F1-Score on ``reliable'' label for, respectively, intra and inter subject testing on a private dataset. Additionally, the combination of the supervised and unsupervised methods can produce semi-supervised methods, such as the method of Feli et al. \cite{ppg-sqa-6}, which feeds features to a semi-supervised one class \gls{SVM}, training it only on samples labeled as ``reliable''. Comparing this semi-supervised approach to rule-based, unsupervised, supervised and deep learning methods lead to the proposed method surpassing all of them in terms of F1-Score for ``reliable'' class with 99\% value on a private dataset. However, likewise the supervised works, most datasets and its testing labeling are not public.        

Another characteristic of the \gls{PPG} \gls{SQA} literature is the existence of the \gls{CA} identification problem. The \gls{CA} is the presence of an anomalous cardiac rate or rhythm without a physiological reason \cite{arrhythmia-1}. This medical condition is an obstacle in the design of \gls{SQI}, since, in contrast to arrhythmic individuals, normal cardiac signals are periodic. Some features assume that the signal is periodic. This assumption can result in signals with arrhythmia being rejected as unreliable signals, leaving patients with the \gls{CA} condition misdiagnosed. Pereira et al. \cite{arrhythmia-2} conducted experiments on a private dataset that contains cases of \gls{AF}, a type or arrhythmia. In this experiment, 40 features used in previous studies fed a \gls{SVM}, which achieved an average accuracy superior to 94\%. That accuracy was far higher than other existing methods, which the researchers also tested. Therefore, abundant feeding a machine learning algorithm with features and training it on datasets with arrhythmia cases already present an improvement in the detection of those special cases.

In sequence, two studies adopted a similar approach to the one mentioned in the past paragraph to attack the arrhythmia problem. In the first study, Pereira et al. \cite{arrhythmia-3} fed several features to three classifiers: \gls{SVM}, \gls{KNN} and decision tree. Similarly to their previous study, experiments on a private dataset demonstrated that the \gls{SVM} was the best of all classifiers and it obtained above 95\% surpassing the methods of other studies. The second study, by Pereira et al. \cite{arrhythmia-4}, also included the \gls{SVM} method, but added to the experiment deep learning models. The study contemplated both one-dimensional (1D) and bidimensional (2D) deep learning models, with the first receiving the raw signal and the second receiving its Cartesian plot image. Experiments on private data with presence of \gls{AF} showed that the ResNet18 model was the best, with 98.5\% accuracy. That last study highlighted that deep learning models have the potential to surpass conventional methods even with the presence of arrhythmic events. The next section further explores the use of deep learning in the literature.  


{
\newcommand{\projectionsWidth}{0.125\textwidth}
\begin{figure*}[ht!]
    \centering
%    \bgroup
%    \setlength{\tabcolsep}{1mm}
%    \def\arraystretch{2}
%    \adjustbox{max width=0.92\textwidth}{%
%    \begin{tabular}{cccc}
%    \multicolumn{4}{c}{\includegraphics[width=0.95\textwidth, trim={2cm 0cm 2cm 0cm}]{signal.png}} \\
%        \fbox{\includegraphics[width=\projectionsWidth]{GramianAngularFieldDifference.png}}
%         & \fbox{\includegraphics[width=\projectionsWidth]{GramianAngularFieldSummation.png}}
%         & \fbox{\includegraphics[width=\projectionsWidth]{MarkovTransitionField.png}}
%         & \fbox{\includegraphics[width=\projectionsWidth]{RecurrencePlot.png}}\\
%        \fbox{\includegraphics[width=\projectionsWidth]{PoincatePlotLogarithmGrid.png}}
%        & \fbox{\includegraphics[width=\projectionsWidth]{MultiscaleMarkovTransitionField.png}}
%        & \fbox{\includegraphics[width=\projectionsWidth, height=\projectionsWidth]{ShortTimeFFT.png}}
%        & ~ \\
%    \end{tabular}
%    }
%    \egroup
%     \caption[A signal and its various projection obtained by several methods.]{A signal and its various projection obtained by several methods. In the first line, from the left to the right, the methods are: Gramian Angular Diference Field~\cite{gaf-mtf-1}, Gramian Angular Summation Field~\cite{gaf-mtf-1}, Markov Transition Field~\cite{gaf-mtf-1} and Recurrence Plot~\cite{rp-1}. The methods of the second line are, from the left to the right: Poincaré Plot Density Map~\cite{ecg-6}, Multiscale Markov Transition Field~\cite{imaging-6} and Short Time Fourier Transform Spectogram~\cite{STFT, imaging-1}.}
    \subfigure[]{
        \includegraphics[width=0.8\textwidth]{signal.png}
        \label{fig:projections:signal}
    }\\
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{GramianAngularFieldDifference.png}
        \label{fig:projections:GramianAngularFieldDifference}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{GramianAngularFieldSummation.png}
        \label{fig:projections:GramianAngularFieldSummation}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{MarkovTransitionField.png}
        \label{fig:projections:MarkovTransitionField}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{RecurrencePlot.png}
        \label{fig:projections:RecurrencePlot}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{PoincatePlotLogarithmGrid.png}
        \label{fig:projections:PoincatePlotLogarithmGrid}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{MultiscaleMarkovTransitionField.png}
        \label{fig:projections:MultiscaleMarkovTransitionField}
    }
    \subfigure[]{
        \includegraphics[width=\projectionsWidth]{ShortTimeFFT.png}
        \label{fig:projections:ShortTimeFFT}
    }\\

       \caption[A signal and its various projection obtained by several methods.]{
           A signal~\subref{fig:projections:signal} and its various projection obtained by several methods. They are: 
           \subref{fig:projections:GramianAngularFieldDifference}~Gramian Angular Diference Field~\cite{gaf-mtf-1}, 
           \subref{fig:projections:GramianAngularFieldSummation}~Gramian Angular Summation Field~\cite{gaf-mtf-1}, 
           \subref{fig:projections:MarkovTransitionField}~Markov Transition Field~\cite{gaf-mtf-1} and 
           \subref{fig:projections:RecurrencePlot}~Recurrence Plot~\cite{rp-1}, 
           \subref{fig:projections:PoincatePlotLogarithmGrid}~Poincaré Plot Density Map~\cite{ecg-6}, 
           \subref{fig:projections:MultiscaleMarkovTransitionField}~Multiscale Markov Transition Field~\cite{imaging-6} and 
           \subref{fig:projections:ShortTimeFFT}~Short Time Fourier Transform Spectogram~\cite{STFT, imaging-1}.
       }
        \label{fig:literature_projections}
\end{figure*}
}


\subsubsection{Signal quality assessment using deep learning}
\label{sec:deep_learning}

Methods based on \gls{DL} have demonstrated the potential to achieve a higher accuracy when compared with feature-based models, even in the presence of \gls{CA} \cite{arrhythmia-3}. On one hand, differently of hand-crafted features, \gls{DL} automatically extracts features from the input signal, creating models that are adaptable to different dataset training contexts. Additionally, a high quality dataset can provide resources for the \gls{DL} model to be robust to variations on the signal conditions. On the other hand, not only it creates a black-box that does not explain the reasons why the model attributed a certain \gls{SQI}, but also requires large amounts of data to properly adjust the model parameters. Despite this, \gls{DL} is worth exploring since it can provide the accuracy and robustness that the medical applications require. 

In this context, several studies proposed the application \glspl{CNN} receiving the one-dimensional input signal. For instance, Naeini et al. \cite{deep-learning-1} designed a 1D \gls{CNN} to extract a binary \gls{SQI}. Tests on a private dataset with data of three devices lead to 85\% F1-score for the ``reliable'' class. Alike, Zanelli et al. \cite{deep-learning-2} employed a self-made 1D \gls{CNN}, but examined the effect of transfer learning as well. They conducted the experiment on three private datasets. It starts training the model mostly on one dataset, in which it achieved an accuracy of 99.8\%. Then, for each remaining database, they fine-tuned the the model with little training and tested on it. This procedure resulted on the 93\% and 81\% accuracies on the second and third datasets, respectively. Additionally, the model trained solely on the second database scored lower, 86\%. Those results indicate that not only 1D \glspl{CNN} can achieve high accuracy in \gls{SQA} but also it is possible to transfer its learned features over different databases to improve its performance.

In a different light, some works go beyond the simple application of such \glspl{CNN}. The research of Lucafo et al. \cite{deep-learning-3}, per example, introduced a hybrid-model for quality assessment, which combines a 1D \gls{CNN} with a rule-based approach. This rule can bypass the utilization of the \gls{CNN} by verifying if the min-to-max distance of the signal is less than an threshold. They determined the threshold by methods such as Last Value Thresholding and Nearest Value Thresholding. The researchers did it to avoid the unnecessary power demanded by the \gls{DL} model. The method proved to be functional since it avoided the usage of the \gls{CNN} for 3.27\% of the input samples, while maintaining similar prediction scores if compared to the \gls{CNN} without the rule component. Therefore, combining the 1D \gls{CNN} with other methods can achieve particular advantages.

Besides the 1D \gls{CNN}s mentioned at this point, works explored the use of alternative \gls{DL} models for the \gls{SQA} of \gls{PPG}. An example of this is recurrent networks, as we can see in the work of Gao et al. \cite{deep-learning-4}, in which they proposed the application of a long-short-term memory network for real time \gls{SQA}, giving a \gls{SQI} for each point in the signal. For the experiments, they labeled private and public datasets by applying Blind Source Separation to generate from each \gls{PPG} signal one high-quality signal and one low-quality signal. When compared to baseline \gls{SQI}s and existing models, it achieved competitive accuracy, while being light-weighted and enoughly fast to predict in real-time. Another example of alternative model is the combination of a stack denoising auto-encoder and a \gls{MLP}, as seen in the work of Singha et al. \cite{deep-learning-5}. This model achieved 95\% accuracy on a private dataset, better than baseline classifiers. A final example is the application of 2D \glspl{CNN} through the projection of the signal into a image using \glspl{GAF} by Naeini et al. \cite{deep-learning-6}. Even though three 2D \glspl{CNN} achieved above 90\% accuracy, F1-Score, and \gls{ROC AUC} scores on a private dataset, a proposed 1D \gls{CNN} overperformed all of them. Thus, several approaches show results that compete with 1D \glspl{CNN}. The usage of 2D \glspl{CNN}, in particular, increased in interest in this last decade.





\subsubsection{Methods based on time series imaging}
\label{sec:imaging}

Several works in the literature propose transforming the input signal into an image and then feeding it to a \gls{CV} model. Figure \ref{fig:literature_projections} shows some of those transformations. Some works used 2D \gls{CNN}s as a classifier of the \gls{SQI}. One of those, Chen et al. \cite{imaging-1}, proposed the construction of a short-time Fourier transform spectrogram as the signal transformation method. The researchers annotated the VitalDB database~\cite{vitaldb-dataset} and, then, conducted experiments that lead to the proposed method achieving a better accuracy than four chosen baseline models. Its value was 98.3\%. 
Chatterjee et al. \cite{imaging-2} transformed \gls{PPG} signals into quantum pattern recognition images for \gls{SQA}. This approach achieved 98.3\% accuracy on the University of Queensland vital signs dataset \cite{queensland-dataset} using labels produced by the researchers, which outperformed both baseline models and a standard \gls{DL} method. Despite the high accuracy reported by Chatterjee et al. \cite{imaging-2}, their study presents reproducibility challenges because the quality labels are not publicly available.
Similarly, Roh et al. \cite{imaging-3} embedded the signal into an \gls{RP} matrix for \gls{PPG} \gls{SQA} to capture non linear dynamics.
Using a private dataset, the method proposed by Roh et al. \cite{imaging-3} achieved 97.5\% accuracy. These results demonstrate that transforming time series into images is a promising approach for \gls{SQA} tasks within the current literature.



One particular method present in the \gls{SQA} literature is the time series matrix embedding, which encodes time relationships of the original signal into a square matrix. For instance, Freitas et al.~\cite{imaging-4} fed a \gls{ViT} with a \gls{RP} or a \gls{MTF}, achieving, respectively, 89.9\% and 90.3\% accuracy on a private dataset. Freitas et al. \cite{imaging-5} also fed images with a \gls{ViT}, but used \glspl{GAF}. The proposed approach reached 92.2\% accuracy on a private dataset. Liu et al. \cite{imaging-6} proposed to input multiscale \gls{MTF}, a \gls{MTF} version which concatenates the signal first and second derivatives, to a self-made \gls{CV} model. Experiments with pre-training on the MIMIC-III and UCI databases, and fine-tuning and testing on the Queensland dataset resulted in 99.1\% accuracy for binary classification. Thus, the combination of a generic \gls{CV} model with a projection method, such as \gls{RP}, \gls{GAF} or \gls{MTF}, can achieve a decent accuracy, while it is possible to apply the multiscaling technique to improve some of those projections accuracy. 


Accurate identification of \gls{PPG} sequences contaminated with artifacts is crucial for enabling reliable smart health applications. In this regard, \gls{ML} techniques have facilitated significant advancements. Although labeled datasets remain scarce, supervised learning models are more frequently adopted than their unsupervised counterparts, with \gls{SVM} and \gls{CNN} being the most widely employed. While feature-engineered and deep learning methods demonstrate similar performance in some scenarios, deep learning may be more advantageous for overcoming the limitations of manual feature engineering. In the current literature, there is a clear need for a standardized experimental framework to validate 2D \gls{DL} approaches against 1D \gls{ML} methods. This work addresses that gap by evaluating various time-series methods to provide a comprehensive study of the field.



\subsection{Contributions of this work}
\label{sec:my_work}

This paper presents a quality assessment framework for \gls{PPG} signals, providing several key contributions. First, it introduces a novel method for encoding time series into 2D images by aggregating various projections into a composite hyperspectral tensor. This approach assumes that such aggregation yields a richer feature representation than a single projection. Second, this work evaluates the approach across a diverse range of \gls{CV} models to identify which architectures best complement time-series matrix embeddings, a rarely scope explored in previous literature. Third, the study explores the application of transfer learning using the ImageNet dataset within the \gls{SQA} domain. Finally, to address the lack of transparency in the field, all experiments are conducted on the publicly available \gls{BUTPPG} dataset~\cite{butppg}, ensuring high reproducibility and providing a standard for future comparative studies.




\section{Proposed Method}
\label{sec_proposed_method}

Section~\ref{sec:related} presented several works in the \gls{SQA} literature employed the time series imaging method as a manner to allow \gls{CV} models to be used in the \gls{SQI} estimation.
Figure~\ref{fig:projections:projection-based-approach} presents a generic framework of such method, which we qualify as ``projection-based''.
It first converts the 1D signal into a 2D projection and afterwards feeds it to a \gls{CV} model.
Finally the \gls{CV} estimates the \gls{SQI}. The framework can use several projection algorithms, but we restrain our scope to four time series embedding methods presented by this section: \gls{GAF}, which generates the variants \gls{GASF} and \gls{GADF}; \gls{MTF}; and \gls{RP}. Moreover, the aggregation of these projections is which we call \gls{PMix}.

\begin{figure}[t!]
    \centering
    \adjustbox{width=\columnwidth}{
    \includegraphics{projection-based-approach.png}
    }
    \label{fig:projections:projection-based-approach}
    \caption{Projection-based time series imaging approach.}
\end{figure}




\subsection{Recurrence Plot}

The work of Eckmann et al.~\cite{rp-1}, of 1987, introduced the \gls{RP} method as a tool for representing visually useful properties and behaviors of a time series. Since then its application expanded to various domains, ranging areas such as earth sciences, finances, engineering, chemistry and physics~\cite{rp-2}. It is also applicable in life sciences, with attempts on identifying the presence of Parkinson's disease~\cite{rp-3}, epileptic seizure~\cite{rp-4}, fetal hypoxia~\cite{rp-5}, and Alzheimer's disease~\cite{rp-6}. Particularly, we employ \gls{RP} in tasks involving cardiological signal processing. Thus, it is likely to be useful for \gls{SQA} of cardiological signals.  

The \gls{RP} represents the occurrence of recurrences between the phase space values of time instant pairs. For this end, the first step is to embed the time series $X=\{x_1,x_2,...,x_n\}$, with $ x_i \in \mathbb{R}$ and $n \in \mathbb{N}$ samples, into a phase space, creating a new time series $S=\{\vec{s_1},\vec{s_2},...,\vec{s_m}\}$, with $ \vec{s_i} \in \mathbb{R}^d$ and $m \in \mathbb{N}$ elements. We can employ the time delays method to represent each element $\vec{s_i}$ of this new sequence $S$ as follows:
\begin{equation}
    \vec{s_i} = (x_i, x_{i + \tau}, x_{i + 2\cdot \tau} ..., x_{i + (d-1) \cdot \tau}),
\end{equation}   
where $d \in \mathbb{N}$ is the dimension and $\tau \in \mathbb{N}$ is the time delay of the phase space. Notice that the length $m$ of the sequence $S$ depends on both $d$ and $\tau$ by the equation $m = n - (d-1) \cdot \tau$. Also notice that this embedding is optional, since the choice of the dimension $d=1$ results in $S=X$, the original time series. Figure~\ref{fig:phase_space} depicts the phase space of the example signal of Figure~\ref{fig:projections:signal}.


\begin{figure}[t!]
    \centering

    \subfigure[Example artificial signal.]{
        \includegraphics[width=0.9\columnwidth]{signal_1.png}
        \label{fig:methodology:signal}
    }

    \subfigure[]{
        \includegraphics[width=0.45\columnwidth, trim={0.5em 1em 2em 2em}, clip]{delay_phase_space.pdf}
        \label{fig:phase_space_A}
    }
    \subfigure[]{
        \includegraphics[width=0.45\columnwidth, trim={0.5em 1em 2em 2em}, clip]{delay_phase_space_recurrences.pdf}
        \label{fig:phase_space_B}
    } 
    \caption{The example signal formed from function $f(t)=\sin(\frac{15 \pi t}{500}) + \frac{t}{500}$.The $t$ variable correspond to the time instant, while the $f(t)$ function gives the magnitude of the signal. On the left, we have the signal of the Figure~\ref{fig:methodology:signal} on the time delay phase space, without temporal information. Its parameters are dimension $d=2$ and delay $\tau=\RPDelay$. On the right, we have almost the same figure, but with the recurrences represented by red lines $\vec{s_i} - \vec{s_j}$ that links the pair of near points that have a distance bellow $\varepsilon=\RPThreshold$. That figure omits the recurrences to the point itself.}
    \label{fig:phase_space}
\end{figure}




\begin{figure}[t!]
    \centering
    \subfigure[Thresholded]{
        \frame{\includegraphics[width=0.47\columnwidth]{RecurrencePlot.png}}
        \label{fig:method:rp:thresholded}
    }
    \subfigure[Unthresholded]{
        \frame{\includegraphics[width=0.47\columnwidth]{RecurrencePlotUnthresholded.png}}
        \label{fig:method:rp:unthresholded}
    }
    \caption[The resulting recurrence plots of the signal in Figure \ref{fig:projections:signal}.]{The resulting recurrence plots of the signal in Figure \ref{fig:projections:signal}, in coherence with the phase space of the Figure \ref{fig:phase_space}. On the left \subref{fig:method:rp:thresholded} we have the thresholded version, while on the right \subref{fig:method:rp:unthresholded} we have the unthresholded version.}
    \label{fig:method:rp}
\end{figure}



Then, the second step is to build a $m \times m$ matrix $RP$ of recurrences where each cell $RP_{i,j} \in \{0,1\}$ represents the presence or the absence of a recurrence in a pair of points $\vec{s_i},\vec{s_j}$ of the phase space $S$. We can represent this concept by measuring the distance $||\vec{s_i} - \vec{s_j}||$ between the points of the pair and verifying if it is smaller than a threshold $\varepsilon \in \mathbb{R}$, as the following equation:
\begin{equation}
    RP_{i,j} = \mathcal{H}(\varepsilon - ||\vec{s_i} - \vec{s_j}||),
\end{equation}
where $\mathcal{H}: \mathbb{R} \mapsto \{0,1\}$ is the Heaviside function. Alternativelly, we can produce an unthresholded version $RP'_{m \times m}$ by attributing to each cell $RP'_{i,j} \in \mathbb{R}$ the points distance:
\begin{equation}
    RP'_{i,j} = ||\vec{s_i} - \vec{s_j}||.
\end{equation}  
Figure~\ref{fig:method:rp} exhibits both $RP$ and $RP'$ of the example signal.


\begin{figure}[t!]
    \centering

    \subfigure[Example of a signal represented in polar coordinate.]{
        \includegraphics[width=0.9\columnwidth, trim={4cm 4cm 4cm 4cm}, clip]{polar.png}
        \label{fig:methodology:polar}
    }

    \subfigure[\gls{GADF}]{
        \frame{\includegraphics[width=0.47\columnwidth]{GramianAngularDifferenceField.png}}
        \label{fig:method:gaf:gadf}
    }
    \subfigure[\gls{GASF}]{
        \frame{\includegraphics[width=0.47\columnwidth]{GramianAngularSummationField.png}}
        \label{fig:method:gaf:gasf}
    } 
    \caption{The example signal corresponding \acrlong{GADF} \subref{fig:method:gaf:gadf} and \acrlong{GASF} \subref{fig:method:gaf:gasf}.}
    \label{fig:method:gaf}
\end{figure}


\begin{figure*}[t!]
    \centering

    \subfigure[Signal segments.]{
        \includegraphics[width=0.46\textwidth, trim={2em 3em 2em 4em}, clip]{QuantileBins.png} 
        \label{fig:method:mtf_signals}
    }
    \subfigure[MTF transitions Markov chain.]{
        \includegraphics[width=0.28\textwidth, trim={1em 1em 1em 1em}, clip]{MarkovChain.pdf}
        \label{fig:method:markov_chain}
    }
    \subfigure[MTF projection]{
        \frame{\includegraphics[width=0.205\textwidth]{MarkovTransitionField.png}}
        \label{fig:method:mtf}
    } 
    
    \caption{Original signal segmented into quantile bins \subref{fig:method:mtf_signals}, the Markov chain representing the transitions of the signal depicted in Figure~\ref{fig:method:mtf_signals} \subref{fig:method:markov_chain}, and the \gls{MTF} of the example signal with the number of quantile bins $m=8$ \subref{fig:method:mtf}.}
    \label{fig:method:mtf_method}
\end{figure*}



\subsection{Gramian Angular Field}

The work of Oates et al.~\cite{gaf-mtf-1} introduced the \gls{GAF} method. This method, in summary, encodes the signal into angular relationships between pair of points. The first step to do this is to convert the signal $X=\{x_1,x_2,\cdots,x_n\}$, with $x_i \in \mathbb{R}$, $n \in \mathbb{N}$ samples, and time instants $\{t_1,t_2,...,t_n\}$, into a polar coordinate series $P=\{p_1,p_2,\cdots,p_n\}$ with $p_i \in \mathbb{R}$. One manner to do that is to associate the time $i \in \mathbb{N}$ to the radius $r_i \in \mathbb{R}$ and the value $x_i \in \mathbb{R}$ to the angle by the inverse of the cosine as follows:
\begin{align}
\begin{split}
    p_i(r_i, \phi_i) & = f_{\sphericalangle}(t_i, x_i) \\
        & = 
        \begin{cases} 
            \phi_i = \arccos(x_i), & -1 \leq x_i \leq 1\\
            r_i = \frac{t_i}{N},     & N \in \mathbb{R}
        \end{cases},
\end{split}
\end{align}
where $N$ is a rescaling factor. Notice that it might be necessary to rescale the signal to fit each $x_i$ in the range $[-1,1]$. Figure~\ref{fig:methodology:polar} shows the application of the function $f_{\sphericalangle}$ over the example signal. The polar coordinate system has one property of interest: the $f_{\sphericalangle}: \mathbb{N} \times \{x \in \mathbb{R}| -1 \leq x \leq 1\} \mapsto \mathbb{R} \times \{\phi \in \mathbb{R}| 0 \leq \phi \leq \pi \}$ is bijective, since it has the inverse function 
\begin{equation}
f_{\sphericalangle}^{-1}(r_i, \phi_i)=(r_i \cdot N, \cos(\phi_i))=(t_i,x_i).
\end{equation}
This indicates that the application of the function $f_{\sphericalangle}$ does not result in loss of information.





{
\newcommand{\NoisesImageWidth}{0.2\textwidth}
\begin{figure*}[t!]
    \centering
    {
    \def\arraystretch{1}
    \setlength{\tabcolsep}{2pt}
    \adjustbox{max width=\textwidth}{
        \begin{tabular}{ccccccc}
            ~
            & Base signal
            & Gaussian Noise
            & Salt-and-Pepper
            & Baseline Wander 
            & HFN
            & Local Noise \\
            
            \rotatebox{90}{\hspace{1em} Signal}
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(base).png}
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(gaussian).png}
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(salt_and_pepper).png} 
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(baseline_wander).png} 
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(low_frequency_noise).png} 
            & \includegraphics[width=\NoisesImageWidth, trim={5.55em 2.5em 4.45em 2.7em}, clip]{signal(local_noise).png} \\
            
            \rotatebox{90}{\hspace{3em} GADF}
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(base).png}}
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(gaussian).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(salt_and_pepper).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(baseline_wander).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(low_frequency_noise).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularDifferenceField(local_noise).png}} \\
            
            \rotatebox{90}{\hspace{3em} GASF}
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(base).png}}
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(gaussian).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(salt_and_pepper).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(baseline_wander).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(low_frequency_noise).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{GramianAngularSummationField(local_noise).png}} \\
            
            \rotatebox{90}{\hspace{4em} RP}
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(base).png}}
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(gaussian).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(salt_and_pepper).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(baseline_wander).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(low_frequency_noise).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{RecurrencePlotUnthresholded(local_noise).png}} \\
            
            \rotatebox{90}{\hspace{3em} MTF}
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(base).png}}
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(gaussian).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(salt_and_pepper).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(baseline_wander).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(low_frequency_noise).png}} 
            & \frame{\includegraphics[width=\NoisesImageWidth]{MarkovTransitionField(local_noise).png}} 
        \end{tabular}
    }
    }
    \caption[The signal, its impaired versions, and their corresponding 2D projections.]{The signal, its impaired versions, and their corresponding 2D projections. From top to bottom: \acrlong{GADF}, \acrlong{GASF}, \acrlong{RP}, and \acrlong{MTF}.}
    \label{fig:method:noises}
\end{figure*}
}







The second step is to construct the temporal relationship matrix. We can achieve that by two methods that exploit trigonometric properties. One of them is calculating the cosine of the summation of the pairs of angles, constructing the matrix $GASF_{n \times n}$:
\begin{align}
\begin{split}
    GASF_{i,j}     & = \cos(\phi_i + \phi_j) \\
            & = \cos(\phi_i) \cdot \cos(\phi_j) - \sin(\phi_i) \cdot \sin(\phi_j) \\
            & = x_i \cdot x_j - \sqrt{1 - x_i^2} \cdot \sqrt{1 - x_j^2}, \label{eq:gasf:algebraic}
\end{split}
\end{align}
where \gls{GASF} is the final matrix. Due to the inversibility of the $arccos$ function, it is possible to express that calculation without trigonometric operations, as expressed by the equality \ref{eq:gasf:algebraic}. Thus, we can calculate \gls{GASF} using matrix operations, as follows:  
\begin{equation}
    GASF = X^T \cdot X - \sqrt[\circ 2]{\mathds{1}-X^{\circ 2}}^T \cdot \sqrt[\circ 2]{\mathds{1}-X^{\circ 2}},
\end{equation}
where $M^{\circ 2}$ and $\sqrt[\circ 2]{M}$ represents the element-wise square power and square root of the matrix $M$, respectively, and $\mathds{1}$ is a matrix in which all elements are $1$. The other method is analogous, but uses the sine of the difference of the pairs of angles, constructing the following matrix $GADF_{n \times n}$:
\begin{align}
\begin{split}
    GADF_{i,j} & = \sin(\phi_i - \phi_j) \\
        & = \sin(\phi_i) \cdot \cos{\phi_j} - \cos(\phi_i) \cdot \sin(\phi_j) \\
        & = \sqrt{1 - x_i^2} \cdot x_j - x_i \cdot \sqrt{1 - x_j^2},  \label{eq:gadf:algebraic}
\end{split}
\end{align}
Also similarly, by the equality \ref{eq:gadf:algebraic}, we can express \gls{GADF} by matrix operations:
\begin{equation}
    GADF = \sqrt[\circ 2]{\mathds{1} - X^{\circ 2}}^T \cdot X - X^T \cdot \sqrt[\circ 2]{\mathds{1} - X^{\circ 2}}.
\end{equation}
Figure~\ref{fig:method:gaf} illustrates the \gls{GASF} and \gls{GADF} representations of the example signal depicted in Figure~\ref{fig:methodology:signal}.








\subsection{Markov Transition Field}

Oates et al.~\cite{gaf-mtf-1} proposed the \gls{MTF} as well, based on a signal to graph mapping of Campanharo et al.~\cite{mtf-1}. In fact, that mapping is the first step of this method. We map the signal $X=\{x_1,x_2,...,x_n\}$, with $x_i \in \mathbb{R}$, to a graph $G=(N,W)$, with nodes set $N$ and edges weights adjacency matrix $W$. Its nodes $N$ corresponds to $m \in \mathbb{N}$ quantile bins $Q_i \subseteq \{x_i | i \in \{1,2,...,n\}\}$, that is, $|Q_1|=|Q_2|=...=|Q_n|$ and, $\forall q_1 \in Q_1, \forall q_2 \in Q_2, ..., \forall q_n \in Q_n$, we have that $q_1 \leq q_2 \leq ... \leq q_n$. In other words, those quantiles bins separate the signal $X$ into bands $Q_i$ with equal amount of samples $x_i$. Figure~\ref{fig:method:mtf_signals} pictures this concept for the example signal. The other graph component, its edges, are directed and corresponds to the probability of a sample $x_{k+1}$, consecutive to a uniformly randomly chosen sample of a certain quantile $x_k \in Q_i$ (must have a consecutive), belonging to a certain quantile $Q_j$. Those edges are akin to transitions of first-order Markov chains, since the probabilities summation of the transitions that sources from a state is always equal to $100\%$. We can express the adjacency matrix $W_{m \times m}$ as follows:  
\begin{equation}
    W_{i,j} = \frac{
            \sum\limits_{x_k \in Q_i, x_{k+1} \in X} f_{in}(x_{k+1}, Q_j)
        }{
            \sum\limits_{Q_l \in Q}\sum\limits_{x_k \in Q_i, x_{k+1} \in X} f_{in}(x_{k+1}, Q_l) 
        },
\end{equation}
where
\begin{equation}
    f_{in}(x,Q) = \begin{cases}
        0, & x \not\in Q \\
        1, & x \in Q
    \end{cases}.
\end{equation}
Figure~\ref{fig:method:markov_chain} depicts the graph of the example signal. This graph $G$ enables a representation  $X' = \{ x'_1,x'_2,\cdots,x'_n \}$  of the input signal $X$ by iteratively selecting a sample from the current quantile node and transitioning to the subsequent node based on the transition probabilities.

Algorithm~\ref{alg:mtf_reconstruction} details this representation, while Figure~\ref{fig:method:mtf_signals} illustrates its application to the example signal. Consequently, the signal conversion to this graph representation is probabilistically reversible. This characteristic ensures that the statistical information of the signal is preserved, even though an exact recovery of the original values is not guaranteed.


\begin{algorithm}[t!]
    \begin{algorithmic}
        \Require Graph $G=(N=\{Q_1, Q_2, ..., Q_m\},W)$
        \Ensure Reconstructed Signal $X'=x'_1,x'_2,...,x'_n$
        \State $Q_{current} \gets Q \in_R N$ 
        \For {$k \in 1,2,...,n$}
            \State $x'_k \gets x \in_R Q_{current}$
            \State $Q_{current} \gets Q_{next}$, with probability $W_{current,next}$
        \EndFor
    \end{algorithmic}
    \caption{The probabilistic signal representation algorithm. }
    \label{alg:mtf_reconstruction}
\end{algorithm}

Since that graph does not retain temporal relationships, the second step of the \gls{MTF} method is to build the matrix $MTF_{n \times n}$:
\begin{equation}
    MTF_{i,j} = W_{u,v} | x_i \in Q_u, x_j \in Q_v,
\end{equation}
\noindent that is, each cell $MTF_{i,j}$ contains the transition probability between the quantiles $Q_u,Q_v$ to which the samples $x_i,x_j$ belong. Figure~\ref{fig:method:mtf} pictures the final result of the method applied to the example signal. 




\subsection{VisionPPG: Computer vision using projection mix}

The main idea of the projection methods is to reflect properties and shapes of the signal into visual patterns. That concept applies too to the inspection of the signal quality. Figure~\ref{fig:method:noises} presents examples of the influence of different type of noises over the projections aspect. On observation is that the presence of low-frequency or high-frequency noises tend to produce, respectively, big or small scale structures. We can see in the effects of that observation in the figure, where the baseline wander figures manifest two to four big structures and the high-frequency noise figures contain small dot-like structures. Another observation is that noises concentrated in a certain region tend to disturb the projection onto a cross-like structure, as we can see in the figure local noise column. We can see that property of ``reflection'' for usual noises, such as Gaussian, and salt and pepper noises. In the figure, the Gaussian noise still preserves the high-level structures and destroys low-level structures, while the salt and pepper noise produces vertical and horizontal lines with void or full colors in places corresponding to, respectively ``salts'' and ``peppers'' of the source signal. Therefore, those projections are likely to be useful in \gls{SQI} tasks for \gls{CV} approaches, since they present the noise and its characteristics as visual patterns that humans can recognize, raising the hypothesis that \gls{CV} could learn them as well.

\begin{figure}[t!]
    \centering

    \subfigure[Proposed \gls{PPG} \gls{SQA} framework.]{
        \includegraphics[width=0.96\columnwidth]{method.png}
        \label{fig:method}
    } 
    \subfigure[Channel adaptations.]{
        \includegraphics[width=0.96\columnwidth]{input_layer.png}
        \label{fig:input_layer}
    }

    \caption{Components of VisionPPG. The whole pipeline is illustrated in \subref{fig:method}, while \subref{fig:input_layer} depicts the four-channel of projection mix is convolved with a $1 \times 1$ convolution filter to produce the three-channel input layer used in the \gls{CV} classifier.}
    \label{fig:visionppg}
\end{figure}

Since each projection possesses unique characteristics, they are combined into an ensemble by fusing all projections into an aggregated tensor. This tensor is analogous to a hyperspectral image because, unlike a typical color image with only three bands, the produced tensor provides additional information layers for each pixel.
This aggregation, termed \gls{PMix}, involves assigning each projection to a particular channel of a single input layer. This structure is then processed through a pointwise convolution operation utilizing a $1 \times 1$ kernel. Formally, the tensor $T_{in}$ with dimensions $p \times n \times m$ used as input for the \gls{CV} models is constructed by stacking the $p$ projections $\{M_1, M_2, \dots, M_p\}$, all with the same dimensions $n \times m$, defined as $T_{in_{k,i,j}} = M_{k_{i,j}}$.
Then, the tensor $T_{in}$ is ``mixed'' into $T_{mix_{q \times n \times m}}$ as follows:
\begin{equation}
    T_{mix_{k,i,j}} = f_{actv}\left(\sum\limits_{l=1}^{p}T_{in_{l,i,j}} \cdot w_{(k,i,j);l}\right),
\end{equation}
where $w_{(k,i,j);l} \in \mathbb{R}$ denotes the weight applied to the connection between the output tensor cell $T_{mix_{k,i,j}}$ and the input tensor cell $T_{in_{l,i,j}}$. The term $f_{actv}: \mathbb{R} \to \mathbb{R}$ represents an activation function, such as ReLU, sigmoid, or Softmax. This relationship can be expressed in terms of the input projections as follows:
\begin{equation}
    \label{eq:mix}
    T_{mix_{k,i,j}} = f_{actv}\left(\sum\limits_{l=1}^{p}M_{l_{i,j}} \cdot w_{(k,i,j);l}\right).
\end{equation}
We can observe that, for each cell with indexes $i,j$, the summation $\sum\limits_{l=1}^{p}M_{l_{i,j}} \cdot w_{(k,i,j);l}$ ``mixes'' the $p$ projections by adding its values $M_{l_{i,j}}$, while attributing different degrees of contribution for each $l$-th projection depending on the weight $w_{(k,i,j);l}$. Then, we can use the $f_{actv}$ function to mainly achieve binary distinguishability, leading to the final tensor value $T_{mix_{k,i,j}}$. Figure~\ref{fig:input_layer} illustrates the implementation framework of that mixture in the context of models pre-trained in three-channeled datasets. That process may require resizing, since pointwise convolution does not change the width and height dimensions.







Based on the \gls{PMix} projection method, this work proposes a new \gls{SQA} framework presented by Figure \ref{fig:method}. First, we transform the signal into four projections using the three before-mentioned algorithms: \gls{GAF}, \gls{MTF}, and \gls{RP}. Afterwards, we aggregate those projections using composition, that is, we assign each of them to a different channel of a new input layer. Then, that layer feeds the computer vision model, which contained weights pre-trained on the ImageNet~\cite{ImageNet} dataset. Finally, that model classifies the signal into a binary \gls{SQI}, which indicates if the signal is ``good'' or ``bad''. 





\section{Experimental Setup} \label{sec:setup}


As with any machine learning task, we require a dataset to supply data for feeding the predictive models during parameter fitting, shaping them to the specific task's domain. In this work, assessing the quality of the signal is framed as a supervised classification problem, which can be described as the task of finding a function that best fits a predefined set of pairs of variables and labels, $(X, y)$. In this context, the pair corresponds to the signal mapped to its quality label, either ``good'' or ``bad''. To train the predictive methods and evaluate their performance in classifying the quality of heartbeat time series, the \gls{BUTPPG}~\cite{butppg} dataset was employed.

The \glsxtrlong{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering at Brno University of Technology. It contains samples of \gls{PPG} signals, their quality labels, and heart rate estimations.
The \gls{PPG} signals were acquired via smartphone-based photoplethysmography, leveraging the device's camera and \gls{LED} source to detect subcutaneous volumetric changes.
Specifically, the researchers recorded the subject's index finger while it covered the camera lens and its \gls{LED} light. For each video frame, they measured the average intensity of the red channel across all image pixels, resulting in a time series of averages. Finally, the signal was inverted. %The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They performed this method of obtaining \gls{PPG} signals 48 times, with the samples distributed equally among 12 subjects—4 measurements per subject. Moreover, the recordings were taken in two situations: one where the subject was seated and remained static, a case in which the quality label ``good'' was likely; and another where the subject was walking, a case likely to result in a ``bad'' recording. This distinction is relevant because the walking condition occurred only once for each subject, resulting in approximately 25\% of the recordings being labeled as ``bad''. Therefore, this dataset is imbalanced, a factor that was addressed in our experiment.

For the definition of signal quality labels, specialists were designated to estimate the heart rate associated with the \gls{PPG} signals using specialized software developed by the researchers. The organizers then compared the specialists' estimates with those provided by a gold standard method, which used an ECG recording as a reference instead of the \gls{PPG} signal. The ECG was manually synchronized by the measurer. If the specialist's measurement had an error of 5 \gls{BPM} or less, the estimate was considered correct. Finally, if 3 out of 5 specialists provided correct estimates, the \gls{PPG} signal quality was labeled as ``good''. Thus, the ``good'' labels in the dataset essentially indicate whether a signal is human-readable.




\subsection{The dataset split}

Machine learning tasks also require the dataset to be split into fragments. One of these is the training dataset, used for fitting the model's parameters. Another is the test dataset, used for evaluating the model's efficiency. An additional split is the validation dataset, used to select the best set of hyperparameters for the trained model or conducting the learning process. A direct way one could achieve the training-test split is randomly partitioning the dataset into both fragments. However, that method never uses the data in the training set for testing and vice-versa. Since the \gls{BUTPPG} is small and it is desirable to reuse data, our experiments defined the training-test splits by using a cross-validation method called \gls{LOSO}, which partitions the dataset into $K$ train-test splits. The $k$-th train-test split assigns the $k$-th segment as the test dataset, leaving the remaining $K-1$ segments as the training dataset. In the case of \gls{BUTPPG}, $K$ equals 12, which corresponds to the number of subjects. In our experiment, the smallest unit of division was the subject, not the individual signals associated with each subject, because having signals of the same subject in both training and testing datasets would introduce biases. This approach has the advantage of increasing the distinction between training and testing samples, since having the same subject in both training and test sets could also introduce biases into the results. Since the dataset is small, this splitting method increases the use of available resources by ensuring every sample is used as a test case at least once. Additionally, the training dataset was further divided using random partitioning to produce a validation dataset of size 3, used for early stopping.

\subsection{The compared techniques}

To evaluate the proposed projection-based framework and identify specific cases of superior performance, it was necessary to involve a large number of machine learning models. Firstly, this work compared the projection-based framework with other time series classification approaches using the Aeon Toolkit~\cite{aeon2024}, with the models listed in Table~\ref{tab:non_cv_list}. Furthermore, the proposed method was combined with a wide variety of classification \gls{CV} models, utilizing the PyTorch Python library~\cite{pytorch}, which supports a diverse range of neural network architectures. These architectures vary from simple convolutional networks to vision transformers. Table~\ref{tab:cv_list} lists all the \gls{CV} models involved in the experiment, which are briefly described in the following subsections of this section.


\begin{table}[t!]
    \centering
    \caption{Non-Computer Vision models list, containing its references.}
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{llc}
        \toprule    
        Classification         & Model                 & Reference             \\
         \midrule    
         Convolution-Based    & Arsenal                & \cite{HIVECOTEV2}        \\
                     & Rocket Classifier            & \cite{RocketClassifier}    \\
         Deep Learning        & Zhao’s CNN Classifier            & \cite{CNNClassifier}        \\
                     & Wang's FCN Classifier            & \cite{FCNClassifier-MLPClassifier}\\
                     & Wang's MLP Classifier            & \cite{FCNClassifier-MLPClassifier}\\
                     & Inception Time Classifier        & \cite{Inception1}\cite{Inception2}\\
                     & Individual Inception Classifier    & \cite{Inception1}\cite{Inception2}\\
                     & LITE Time Classifier            & \cite{LITETimeClassifier}    \\
         Dictionary-Based    & BOSS Ensemble                & \cite{BOSSEnsemble}        \\
                     & Contractable BOSS            & \cite{ContractableBOSS}    \\
                     & Individual BOSS            & \cite{BOSSEnsemble}        \\
                     & Individual TDE            & \cite{TDE}            \\
                     & MUSE                    & \cite{MUSE}            \\
                     & TemporalDictionaryEnsemble        & \cite{TDE}            \\
                     & WEASEL                & \cite{WEASEL}            \\
                     & WEASEL V2                & \cite{WEASELV2}        \\
                     & REDCOMETS                & \cite{REDCOMETS-1}\cite{REDCOMETS-2}\\
         Distance-Based        & Elastic Ensemble            & \cite{ElasticEnsemble}    \\
                     & K-Neighbors Time Series Classifier    & ---                \\
                     & Shape DTW                & \cite{ShapeDTW}        \\
         Feature-Based        & Catch-22 Classifier            & \cite{Catch22Classifier}    \\
                     & Summary Classifier            & ---                \\
                     & TS Fresh Classifier            & \cite{TSFreshClassifier}      \\
         Inverval-Based        & Interval Forest Classifier    & \cite{CanonicalIntervalForestClassifier}\\
                     & DrCIFClassifier            & \cite{HIVECOTEV2}        \\
             & Random Interval Spectral Ensemble Classifier & \cite{RandomIntervalSpectralEnsembleClassifier}\\
                     & Supervised Time Series Forest        & \cite{SupervisedTimeSeriesForest}\\
                     & Time Series Forest Classifier        & \cite{TimeSeriesForestClassifier}\\
                     & Random Interval Classifier        & ---                \\
         Shapelet-Based        & Shapelet Transform Classifier        & \cite{ShapeletTransformClassifier-1}\cite{ShapeletTransformClassifier-2}\\
                     & RDST Classifier            & \cite{RDSTClassifier-1}\cite{RDSTClassifier-2}\\
         Ordinal Classification    & Individual Ordinal TDE        & \cite{OrdinalTDE}        \\
                     & Ordinal TDE                & \cite{OrdinalTDE}        \\
         Other            & Continuous Interval Tree        & \cite{ContinuousIntervalTree}    \\
                     & Rotation Forest Classifier        & \cite{RotationForestClassifier}\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:non_cv_list}
\end{table}



\begin{table}[t]
    \centering
    \caption{\Glsxtrlong{CV} models list, containing its citations.}
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{llcc}
        \toprule    
        Classification         & Model         & Reference             \\
         \midrule    
        Transformer            & Vision Transformer    & \cite{VisionTransformer}    \\
                       & MaxViT        & \cite{MaxViT}            \\
                       & Swin Transformer    & \cite{SwinTransformer}    \\
                       & Swin Transformer V2    & \cite{SwinTransformerV2}    \\
        Residual Net           & ResNet        & \cite{ResNet}            \\
                     & ResNeXt        & \cite{ResNeXt}        \\
                     & WideResNeXt        & \cite{WideResNet}        \\
        Extreme Net           & DenseNet        & \cite{DenseNet}        \\
                     & VGG            & \cite{VGG}            \\
                    & SqueezeNet        & \cite{SqueezeNet}        \\
        Mobile-Oriented        & MNASNet        & \cite{MNASNet}        \\
                    & MobileNet V2        & \cite{MobileNetV2}        \\
                    & MobileNet V3        & \cite{MobileNetV3}        \\
        Efficiency-Oriented    & EfficientNet        & \cite{EfficientNet}        \\
                    & EfficientNet V2    & \cite{EfficientNetV2}        \\
                    & ShuffleNet V2        & \cite{ShuffleNetV2}        \\
        Diverse              & AlexNet        & \cite{AlexNet}        \\
                    & ConvNeXt        & \cite{ConvNeXt}        \\
                     & RegNet        & \cite{RegNet}            \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:cv_list}
\end{table}



\subsubsection{Transformers}

The experiments tested four transformers: \gls{ViT}, \gls{MaxViT}, \gls{SwinT}, and its second version, \gls{SwinTV2}. The \gls{ViT} transforms visual input into a sequence where each element is a linear embedding of subimage patches obtained by partitioning the original image into a grid-like pattern~\cite{VisionTransformer}. Subsequent models build on this base by incorporating additional layers and altering attention mechanisms. For example, the \gls{MaxViT} utilizes architectural blocks that alternate between two self-attention modes: grid attention, which operates with high granularity, and block attention, which operates with low granularity~\cite{MaxViT}. The \gls{SwinT} modifies attention at both the layer level—by merging patches from the previous layer—and at the block level—by shifting self-attention windows to different positions~\cite{SwinTransformer}. The \gls{SwinTV2} introduces several specific improvements over the earlier version~\cite{SwinTransformerV2}.

\subsubsection{Residual nets}

ResNet introduced residual connections, which are links between non-adjacent layers that bypass intermediate layers~\cite{ResNet}. The two variations considered are Wide ResNet and ResNeXt. Wide ResNet expands the original network by increasing the number of channels per block, offering an alternative to increasing layer depth~\cite{WideResNet}. In contrast, ResNeXt employs a multipath approach, aggregating paths through an additive operation~\cite{ResNeXt}. Instead of increasing width and depth, ResNeXt introduces an additional dimension for enhancement.

\subsubsection{Mobile Nets}

This study defines mobile oriented models as neural networks designed specifically to address hardware constraints. We evaluate three architectures: MNASNet~\cite{MNASNet}, MobileNet V2~\cite{MobileNetV2}, and MobileNet V3~\cite{MobileNetV3}. MobileNet V2 incorporates architectural changes to reduce memory usage while maintaining accuracy through the use of inverted residual blocks~\cite{MobileNetV2}. This design alternates high and low channel layers by connecting layers with fewer channels, which reduces the total number of parameters in the block~\cite{MobileNetV2}. MNASNet selects blocks to fit a predefined architectural skeleton to optimize model performance on actual mobile hardware~\cite{MNASNet}. Finally, MobileNet V3 combines these approaches and introduces further refinements, such as the inclusion of the NetAdapt~\cite{NetAdapt} algorithm within the architectural search process~\cite{MobileNetV3}.

\subsubsection{Extreme Nets}

This paper defines extreme nets as neural models that prioritize the optimization of specific architectural concepts such as layer depth, model compression, or residual connectivity.
This category includes VGG~\cite{VGG}, DenseNet~\cite{DenseNet}, and SqueezeNet~\cite{SqueezeNet}. VGG utilizes $3 \times 3$ filters to facilitate deeper architectures by increasing the total number of layers~\cite{VGG}.
DenseNet employs skip connections between all pairs of architectural blocks which positions each layer closer to both the input and the output to enhance performance~\cite{DenseNet}.
SqueezeNet minimizes memory usage through model compression and the introduction of a specialized architectural module that reduces channel density before applying large convolution filters~\cite{SqueezeNet}.
This strategy significantly reduces the parameter count compared to convolutions applied to layers with a large number of channels~\cite{SqueezeNet}.

\subsubsection{Efficient Nets}

\hyphenation{Shuffle-Net}

This paper defines efficiency-oriented networks as architectures designed for optimal resource utilization to maximize performance with minimal parameter counts.
Notable examples include ShuffleNet V2~\cite{ShuffleNetV2}, EfficientNet~\cite{EfficientNet}, and EfficientNet V2~\cite{EfficientNetV2}.
ShuffleNet V2 advances the original ShuffleNet by introducing the channel shuffle operator to facilitate information exchange across channels~\cite{ShuffleNetV2}.
It improves upon its predecessor by incorporating a channel split operation within each block to avoid the use of computationally expensive grouped convolutions~\cite{ShuffleNetV2}.
EfficientNet focuses on model scaling through a compound resizing method that proportionally increases depth, width, and resolution~\cite{EfficientNet}.
This approach creates a highly efficient base model that can be scaled to larger variants while preserving the inherent advantages of the architecture~\cite{EfficientNet}.
EfficientNet V2 builds on this foundation by proposing non proportional scaling and utilizing network architecture search~\cite{EfficientNetV2}.
Furthermore, it introduces progressive learning which involves the gradual increase of dataset regularization during training~\cite{EfficientNetV2}.

\subsubsection{Diverse}

The remaining models not included in the anterior groups were reunited in this category. It includes the following models: AlexNet, ConvNeXt, and RegNet. Introduced in 2012, AlexNet was one of the earliest deep learning models designed to be trained across multiple GPUs, which accelerated the training process and utilized dropout to mitigate overfitting~\cite{AlexNet}. In contrast, ConvNeXt, a modern model from 2022, integrates various convolutional techniques from recent years, such as patchified convolutions, inverted bottlenecks, and grouped convolutions, with the goal of advancing traditional convolutional networks~\cite{ConvNeXt}. On the other hand, RegNet departs from designing individual networks by focusing on creating network families defined by linear parameter spaces, facilitating architectural search within these defined populations~\cite{RegNet}. The experiments in that category encompassed networks with significant variations among them.



\subsection{Defining the hyperparameters}

However, defining the models alone is insufficient, as the selection of their hyperparameters is also required. Hyperparameters are parameters related to the learning process, rather to the model itself. For the Aeon models, the default hyperparameters provided by the library were used for convenience reasons, even though they are not the optimal ones. For the computational vision models, while most hyperparameters were set to their defaults, our experiments employed hyperparameter search for the learning rate, used by the optimization algorithm to search for better hyperparameters than the defaults provided by the PyTorch library. The Optuna library~\cite{optuna} conducted this search by heuristically exploring the parameter space dynamically defined in the user code. Optuna prunes the search-space tree using various methods, and in our experiments, the median pruning method was applied. In this case, the guiding metric for the heuristic search was the accuracy score, defined as the ratio of correct predictions to the total number of samples. It was chosen since maximizing that ratio is desired, as the more correct predictions, the better. The accuracy was measured on a validation dataset of size equal to 2 subjects, created through a simple random split. This functionality allowed us to find a near-optimal combination of parameters without exhaustively testing all possible cases, using the model's validation dataset score as a heuristic.

\subsection{Training strategy}

Given the aforementioned models, dataset, and its divisions, it was necessary to establish the training method for adjusting the models' parameters straightforwardly.
Experiment involved feeding the models by loading the signals data, applying random oversampling before transforming them, as the dataset was unbalanced. After performing the projection transform, our experiment loaded pre-trained model weights provided by PyTorch, which were originally trained on the ImageNet~\cite{ImageNet} dataset.
Transfer learning was employed by utilizing pre-trained ImageNet weights; this leverages the model's existing capacity for low-level feature extraction (e.g., edges and textures) relevant to 2D signal projections.
Following that, we fitted the PyTorch models using the Adam optimization algorithm~\cite{Adam} to minimize the cross-entropy loss function. A reason to use that algorithm is that it surpassed some of the other options present in the PyTorch library when tested in several datasets \cite{Adam}. The implementation of the training strategy performed this optimization cycle with a number of epochs determined by a median-deviation-based early stopping technique, through the assumption that a low dispersion on the last epochs indicates a convergence to a local-optimal in the search space. The formula below gives the score of the $n$-th epoch:
\begin{equation}
EarlyStopScore(n) = med([|l_{n-i} - med([l_{n-i}]_{i=0}^9)|]_{i=0}^{9}),
\end{equation}
\noindent where $l_k$ is the loss value (i.e., cross-entropy loss) of the $k$-th epoch, ${med}$ is the median and $[f(i)]_{i=0}^p$ is the sequence generated by $f(i)$ when varying $i$ from $0$ to $p$. In other words, the formula calculates the median of the absolute deviations of the medians of the last 10 loss values using the central value. If $EarlyStopScore(n) \leq 0.1$, the training stops in the $n$-th epoch. With that established, it remains to determine the metrics to be measured for smoother readability.

\subsection{Performance measurements}

Being established the training procedure, it is needed to choose metrics to evaluate the efficacy of the solution after the training of the model. For these experiments, we can categorize the metrics into two groups: prediction metrics, which measure the quality of the model's signal quality assessments, and benchmarking metrics, which measure resource usage and the model speed. As the prediction metrics, our experiments used the Cohen kappa score, the F1-score, and the precision, considering that they are capable of estimating the quality of binary classification through different perspectives. All of them can be evaluated using confusion matrix values, presented in Table~\ref{tab:experimentos:confusion_matrix}. The Cohen kappa score~\cite{CohenKappa}, in binary classification tasks, measures the agreement between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen kappa score:
\begin{equation} 
acc_o = \frac{TP+TN}{N},
\end{equation}
\begin{equation}
acc_e  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right),
\end{equation}
and
\begin{equation} \label{eq:Cohen kappa}
CohenKappa(R)  = \frac{acc_o - acc_e}{1 - acc_e}, 
\end{equation}  
where $N=TP+TN+FP+FN$ is the total number of samples. For the purpose of aligning this metric with others, we can rescale that metric from $[-1,1]$ to $[0,1]$:
\begin{equation}
CohenKappaRescaled(R) = \frac{CohenKappa(R)+1}{2}.
\end{equation}  
In sequence, the precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher precision implies that the predictor avoided mistakenly labeling ``bad'' signals as ``good'', which is desirable in applications where we do not want to show to the user measurements based on unreliable signals. From the precision and from the recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between precision and recall scores. In our application, it measures the same as the precision plus the recall, which would measure the amount of ``good'' signals that would feed the application. This is an desirable quality when we want to provide constant feedback to the user. The following equations define those metrics:
\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP},
\end{equation}
\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN},
\end{equation}
and
\begin{equation} \label{eq:F1-Score}
F1 = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}.
\end{equation}
Therefore, the Cohen kappa score provides an overall sense of accuracy, the F1-Score suggests the model's usability level, and precision indicates the predictor's reliability.

\begin{table}[h!]
    \centering
    \caption[Binary classification confusion matrix]{Binary classification confusion matrix, where each cell resulting from the intersection between the $i$-th line and the $j$-th colum correspond to the amount of data which was predicted as the $i$-th line label and had the $j$-th column label as the true value.}
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{c|cc}
        \toprule
        \diagbox{Predicted label}{True label} 
             & Positive  & Negative\\
        \midrule
        Positive & \Gls{TP} & \Gls{FP}\\
        Negative & \Gls{FN} & \Gls{TN}\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:experimentos:confusion_matrix}
\end{table}

Regarding the benchmarking metrics, our experiment measured the memory usage of the model in bytes and the inference time (including the 1D-to-2D projection time for projection-based models) in seconds. Memory usage is crucial because practical applications for heart rate estimation often impose hardware constraints that limit allowable memory usage. Additionally, inference time is important for achieving near-instantaneous evaluations, which enhances the application's responsiveness.

\subsection{Overall schema}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{framework.png}
    \caption[The framework of the experiment.]{The framework of the experiment. Red dotted arrows indicates data flow that sources from the \acrlong{BUTPPG} dataset, while the full black lines, labeled with an verb, represent a relationship ``A do B'', where the arrow starts on A and end on B. Notice that the figure presents the training-testing cycle for only one of the twelve folds.}
    \label{fig:framework}
\end{figure*}


Figure~\ref{fig:framework} illustrates the experiment framework applied to each combination of \acrlong{CV} model and projection algorithm. One notable difference from the framework used for non-\gls{CV} models is that the 1D-to-2D conversion acts as a boundary between the dataset and the other components. So the experiment for non-\gls{CV} models is represented using a similar schema by omitting the conversion block. The experiment began with hyperparameter selection, involving the splitting of the \gls{BUTPPG} dataset through a simple division method to subsequently select the optimal learning rate for the \gls{CV} models. With the best learning rates chosen, all models, including non-\gls{CV} models, will be evaluated using the \gls{LOSO} strategy. For each fold, our experiments subjected the model to a training procedure that iterates through epochs of training and validation until early stopping is triggered. The model is then tested to produce the metrics for that fold.




\subsection{Implementation details}

The dataset sourcing procedure was carried out using the PyTorch multithreading data feeding solution, dataloader. This was configured to load batches of size 32 for all \gls{CV} models to make the comparison more uniform, since that variable can change their performance. Prior to loading the batches, the training dataset was balanced using the Imbalanced Learn library~\cite{ImbalancedLearn2017} and its random oversampling method, since it is a open source implementation of an algorithm that equalizes the proportion of labels in the learning process, avoiding an label unbalance that our experiment design hypothesizes that it could overfit the model to a specific label, which is undesirable. The batches were then transformed from 1D signals into 2D images using the projection algorithms of the PyTS library~\cite{PyTS}, which allows the reproducibility of such transformations for being open source. Although the signals are now 2D, their width and height might not be compatible with the original network's input dimensions, especially considering pre-trained weights. To address this issue, the PyTorch resize transform was applied to adjust the width and height. Additionally, a new convolutional layer corresponding to the \gls{PMix} method was incorporated.

The \gls{CV} models were trained using a single NVIDIA\texttrademark RTX 3090 TI. For training, our implementation used the Pytorch implementation of the Adam optimization algorithm~\cite{Adam}, which uses the gradients evaluated by the Pytorch autograd engine~\cite{pytorch}. The loss class (which, in our case, is the torch.nn.CrossEntropyLoss, used for being an open source implementation of the cross entropy loss function) backpropagates the gradients based on the model forward pass errors. For the models testing, our implementation used the Sklearn's metrics~\cite{Sklearn}, since they are open source. For model memory measurement, our implementation counted the summation of the size of each parameter and buffer tensors in the \gls{CV} models, while for the non-\gls{CV} models, our implementation used the \texttt{asizeof} function of the Pympler library~\cite{Pympler}. Finally, we describe the inference time measurement, for which our implementation extracted 500 measurement samples. For the non-\gls{CV} models, our implementation used the \texttt{time} method of the Python's time module, from its standard library, to measure two time instants: the moment right before the model testing predictions, when the model is already trained; and the moment right after those predictions. Our implementation evaluates the time interval between those instants to estimate the inference time of the non-\gls{CV} model. For the \gls{CV} models, our implementation marked the time instants by using CUDA events interface provided by Pytorch, while, before measuring, performing 500 iterations to warm-up the \gls{GPU}.




\section{Results} \label{sec:results}


The results were analyzed by comparing the score metrics of the models and assessing their trade-offs with respect to memory consumption and inference time. The score metrics are presented like the example in Table \ref{tab:experiments:example}, in which the results of each combination of model and projection are shown in the format $mean \pm std$, with values up to three decimal places. Additionally, considering a particular column of mean or standard deviation, it is used a coloring system where the red color represents values worse or equal than the first quartile, the green color highlights values better or equal than the third quartile, and the blue color paints values in between the other colors. Given the large number of models considered, the analysis was organized into sections. First, each section focused on one of the \gls{CV} model families listed in Table~\ref{tab:cv_list}: Transformers, Residual Nets, Mobile-Oriented, Extreme Nets, Efficiency-Oriented, and Diverse. Within each category, the analysis identified the best combinations of model variants and projection methods. Subsequently, the top non-\gls{CV} models from the Aeon toolkit library were selected. Finally, the overall best choices were determined, and differences between the projection methods were discussed.


\begin{table}[h!]
    \centering
    \caption{Example of the score-displaying system.}
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{llll}
        \toprule
        Model & Projection & Example metric 1 & Example metric 2 \\
        \midrule
        Example model 1 & Example projection 1 
            & \textcolor[rgb]{0.0,0.7,0.0}{0.999} $\pm$ \textcolor[rgb]{0.3,0.3,0.5}{0.333} 
            & \textcolor[rgb]{0.3,0.3,0.5}{0.777} $\pm$ \textcolor[rgb]{0.0,0.7,0.0}{0.111} 
        \\
        Example model 2 & Example projection 2 
            & \textcolor[rgb]{0.3,0.3,0.5}{0.333} $\pm$ \textcolor[rgb]{1.0,0.2,0.2}{0.999} 
            & \textcolor[rgb]{0.0,0.7,0.0}{0.999} $\pm$ \textcolor[rgb]{0.3,0.3,0.5}{0.333} 
        \\
        Example model 3 & Example projection 3 
            & \textcolor[rgb]{0.3,0.3,0.5}{0.777} $\pm$ \textcolor[rgb]{0.0,0.7,0.0}{0.111} 
            & \textcolor[rgb]{1.0,0.2,0.2}{0.111} $\pm$ \textcolor[rgb]{0.3,0.3,0.5}{0.777} 
        \\
        Example model 4 & Example projection 4 
            & \textcolor[rgb]{1.0,0.2,0.2}{0.111} $\pm$ \textcolor[rgb]{0.3,0.3,0.5}{0.777} 
            & \textcolor[rgb]{0.3,0.3,0.5}{0.333} $\pm$ \textcolor[rgb]{1.0,0.2,0.2}{0.999} 
        \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:experiments:example}
\end{table}





\subsection{Analysis by computer vision model family}
 
This analysis covers each \gls{CV} model family listed in Table~\ref{tab:cv_list}.
 
\subsubsection{Vision Transformers}

One metric table was generated for each type of transformer.
% Vision Transformer
Table \ref{tab:Averages_of_VisionTransformer} presents the \gls{ViT} scores, with variants categorized as Base (B), Large (L), or Huge (H) in parameter size and patch sizes of 14, 16, or 32. The table shows that the \gls{PMix} and \gls{RP} projection methods achieved the best scores across all metrics. In most cases, \gls{PMix} was equal to or better than \gls{RP}, except for the \mbox{H 14} variant, where \gls{RP} was superior. Among the combinations of variants and projections, \gls{RP} and \gls{PMix} with \mbox{B 16} and \mbox{L 16}, as well as \gls{PMix} with \mbox{B 32} and \mbox{L 32}, yielded the best scores.
% MaxViT
Table \ref{tab:Averages_of_Maxvit} shows the \gls{MaxViT} scores.  For this model, the \gls{PMix} method achieved the highest scores for the Cohen Kappa and precision metrics, while the \gls{RP} surpassed it for the F1-Score, despite \gls{PMix} having the smallest dispersion for that metric.

% Swin Transformer
Table \ref{tab:Averages_of_SwinTransformer} exhibits the \gls{SwinT} scores, with variants categorized as Base (B), Small (S), or Tiny (T) based on parameter count. The \gls{RP} method achieved the best scores for the B and S variants, while the \gls{PMix} method resulted in better scores for the T variant. Specifically, the \gls{PMix} method with the T variant attained the highest Cohen Kappa and precision scores, but ranked second for the F1-Score, which was surpassed by the \gls{RP} method with the S variant.
% Swin Transformer V2
Table \ref{tab:Averages_of_SwinTransformerV2} displays the \gls{SwinTV2} scores, with variants categorized as Base (B), Small (S), or Tiny (T). The \gls{PMix} method achieved better scores for the B and S variants, while the \gls{RP} method performed better for the T variant, despite \gls{RP} having the largest dispersion for the F1-Score in this case. Specifically, the \gls{PMix} method with the S variant resulted in the highest Cohen Kappa and F1 scores, and the second-best precision, where the \gls{RP} method with the T variant was superior.

% Inter-Family Comparison
% When considering all Tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformerV2}, the \gls{RP} and \gls{PMix} methods with \mbox{\gls{ViT} B 16} and \mbox{\gls{ViT} L 16}, as well as \gls{PMix} with \mbox{\gls{ViT} B 32} and \mbox{\gls{ViT} L 32}, and the \gls{SwinTV2} S with \gls{PMix}, generally achieved better scores. The benchmarking metrics for these combinations are summarized next. Table~\ref{tab:Memory_of_Transformers} shows that the \gls{SwinT} V2 S variant uses considerably less memory than the \gls{ViT} variants. Therefore, the \gls{SwinT} V2 S with \gls{PMix} can achieve high scores while utilizing less memory. However, Figure~\ref{fig:Time_of_Transformers} indicates that the \gls{SwinT} V2 S variant has a slower inference speed compared to the \gls{ViT} variants. Among the \gls{ViT} variants, the \mbox{B 32} variant was the fastest, suggesting that the combination of \gls{PMix} with \gls{ViT} B 32 can produce high scores with lower inference time. Therefore, we select the following methods for this section:
% \begin{itemize}
%     \item \gls{SwinT} V2 S with \gls{PMix};
%     \item and \gls{ViT} B 32 with \gls{PMix}. 
% \end{itemize}


\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the Vision Transformer variants.}
    \label{tab:Averages_of_VisionTransformer}
    \adjustbox{max width=\columnwidth,center}{
    \begin{tabular}{lllll}
    \toprule
    Model & Projection & Cohen Kappa & F1 Score & Precision \\
    \midrule
    ViT: B 16 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.562} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.167} \\
    & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.518} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.040} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.773} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.175} \\
    & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    ViT: B 32 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.583} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.844} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.074}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.785} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
    & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.515} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.207} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.790} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.167} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
    & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.883} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    ViT: H 14 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129}} \\
    & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.477} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.708} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
    & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.943} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
    & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.222} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.903} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.146} \\
    ViT: L 16 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.246} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.873} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.117} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.812} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.188} \\
    & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.594} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.254} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.851} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.118} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.736} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.284} \\
    & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    ViT: L 32 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.674} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.119} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.819} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} \\
    & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.612} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.196} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.828} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.141} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.811} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} \\
    & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.943} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
    & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
    \bottomrule
    \end{tabular}
    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the MaxViT variants.}
    \label{tab:Averages_of_Maxvit}
    \adjustbox{max width=\columnwidth,center}{
        \begin{tabular}{lllll}
        \toprule
        Model & Projection & Cohen Kappa & F1 Score & Precision \\
        \midrule
        MaxViT & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.653} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.261} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.857} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.132} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.192} \\
        & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.544} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.190} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.706} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.186} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.788} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.222} \\
        & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.225} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
        & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.169}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.921} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.098}} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
        \bottomrule
\end{tabular}
    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the SwinTransformer variants.}
    \label{tab:Averages_of_SwinTransformer}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
SwinT: B & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.625} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.226} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.110} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.179} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129}} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.883} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
SwinT: S & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.696} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.236} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.838} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.160} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.198} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.226} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.820} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.181} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.194} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.234} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.919} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
SwinT: T & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.571} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.216} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.765} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.187} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.198} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.514} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.166} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.110} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.736} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.727} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.261} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.897} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.127} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.195} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
\bottomrule
\end{tabular}
    }
\end{table}




\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the SwinTransformerV2 variants.}
    \label{tab:Averages_of_SwinTransformerV2}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
SwinTV2: B & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.162} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.860} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.085} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.764} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.132} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.222} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.931} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.127} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
SwinTV2: S & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.611} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.239} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.829} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.134} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.785} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.183} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.195} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.097} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.140} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
SwinTV2: T & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.674} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.119} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.819} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.842} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.210} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.901} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.152} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.115}} \\
 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.129} \\
\bottomrule
\end{tabular}
    }
\end{table}



\subsubsection{Residual Nets}

The experiments produced three score tables.
% ResNet
Table~\ref{tab:Averages_of_ResNet} presents the ResNet scores for variants with 18, 34, 50, 101, and 152 layers. Among these, the \gls{PMix} and \gls{RP} methods outperformed the other projection methods. Specifically, the \gls{PMix} method achieved the best scores when combined with the 50 and 101-layer variants.
% ResNeXt
Table~\ref{tab:Averages_of_ResNeXt} displays the ResNeXt scores for variants with 50 or 101 layers, cardinality of 32 or 64, and bottleneck width of 4 or 8. Among these, the \gls{PMix} and \gls{RP} methods achieved the best scores. Notably, the \gls{RP} method with the \mbox{ResNeXt 101 $32\times 8d$} variant achieved the highest scores.
% Wide ResNet
Table \ref{tab:Averages_of_WideResNet} lists the Wide ResNet scores for variants with 50 or 101 layers and a widening factor of 2. The \gls{PMix} and \gls{RP} methods consistently performed better across all metrics. Notably, the \gls{PMix} method with the Wide \mbox{ResNet 101-2} variant achieved the best scores for Cohen kappa and F1-Score, and the second-best score for precision.
% Inter-Family Comparison
Observing Tables~\ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt}, and \ref{tab:Averages_of_WideResNet} together reveals that the \mbox{Wide ResNet 101-2} with \gls{PMix} was the top-performing combination in terms of scoring.
%However, this combination had the highest memory usage, according to Table~\ref{tab:Memory_of_Residual Nets}, and was the fourth slowest in inference time, as seen in Figure~\ref{fig:Time_of_ResNet based}. An alternative with nearly the second-best scores but significantly lower memory usage and inference time is the \mbox{ResNet 50} with \gls{PMix}. Thus, the two methods below were chosen for this section:
% \begin{itemize}
%     \item ResNet 50 with \gls{PMix}; 
%     \item and Wide ResNet 101-2 with \gls{PMix}. 
% \end{itemize}




\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the ResNet variants.}
    \label{tab:Averages_of_ResNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
ResNet: 101 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.558} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.223} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.848} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.106} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.708} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.279} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.825} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.074}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.244} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.851} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.147} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.867} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.185} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
ResNet: 152 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.609} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.266} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.874} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.123} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.291} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.557} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.168} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.787} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.135} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.785} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.183} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.234} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.925} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.089} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.894} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.149} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
ResNet: 18 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.661} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.256} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.807} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.172} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.182} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.547} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.188} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.148} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.926} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.908} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.109} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.176} \\
ResNet: 34 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.599} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.210} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.148} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.165} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.098} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.725} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.142} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.225} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.127}} \\
ResNet: 50 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.510} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.254} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.772} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.178} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.681} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.293} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.470} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.067} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.110} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.715} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.818} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.226} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
\bottomrule
\end{tabular}
    }
\end{table}




\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the ResNeXt variants.}
    \label{tab:Averages_of_ResNeXt}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
ResNeXt: 101; 32x8d & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.271} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.853} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.179} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.847} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.204} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.162} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.844} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.102} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} \\
 & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.758} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.230} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.877} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.145} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
ResNeXt: 101; 64x4d & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.479} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.188} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.752} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.159} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.708} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.169} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.511} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.580} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.217} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.758} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.204} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.856} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.149} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.144} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.222} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.915} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.115} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.903} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.146} \\
ResNeXt: 50; 32x4d & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.542} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.144}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.794} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.161} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.158} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.162} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.860} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.085}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.764} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.132}} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.282} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.870} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.183} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.847} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.204} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.792} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.234} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.919} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
\bottomrule
\end{tabular}
    }
\end{table}





\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the WideResNet variants.}
    \label{tab:Averages_of_WideResNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
WiResNet: 101-2 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.625} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.226} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.826} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.158} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.803} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.508} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.110} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.702} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.189} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.194} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.842} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.210} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.905} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.159} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.970}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.101}} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.101}} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.967}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.078}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
WiResNet: 50-2 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.486} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.117} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.737} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.713} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.196} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.550} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.145} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.786} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.142} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.773} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.175} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
\bottomrule
\end{tabular}
    }
\end{table}






\subsubsection{Mobile Nets}

Three metric tables were generated for mobile-oriented family of \gls{CV} models.
%MNASNet
Table~\ref{tab:Averages_of_MNASNet} records the scores obtained by the MNASNet variants, which can have depth multipliers of 0.5, 0.75, 1.0, or 1.3, affecting the number of channels. Notably, the combination of \mbox{MNASNet 1.0} with \gls{PMix} achieved the best scores across all metrics.
%MobileNet V2
Table~\ref{tab:Averages_of_MobileNetV2} presents the scores for MobileNet V2. The \gls{RP} projection achieved the best Cohen kappa and precision scores, while \gls{PMix} obtained the highest F1-Score. However, \gls{RP} demonstrated greater consistency, with lower variability in results compared to the standard deviations of \gls{PMix}.
%MobileNet V3 
Table~\ref{tab:Averages_of_MobileNetV3} lists the MobileNet V3 variants, which include Small and Large configurations in terms of resource usage. Notably, \gls{PMix} with the Large variant achieved the best Cohen kappa and precision scores, while \gls{RP} with the Small variant excelled in the F1-Score metric.
% Inter-Family Comparison
% From the Tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNetV2} and \ref{tab:Averages_of_MobileNetV3}, the combination of \mbox{MNASNet 1.0} with \gls{PMix} emerges as the overall best case. This combination demonstrates a competent inference time when comparing to the other models in the category, as shown in Figure~\ref{fig:Time_of_Mobile nets}, but its memory consumption was not among the best models in the category, according to Table~\ref{tab:Memory_of_Mobile-Oriented}. Nonetheless, we elect only one method as the best models of this section:
% \begin{itemize}
%     \item MNASNet 1.0 with \gls{PMix}.
% \end{itemize}


\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the MNASNet variants.}
    \label{tab:Averages_of_MNASNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
MNASNet: 0.5 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.513} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.211} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.812} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.552} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.367} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.480} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.133} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.798} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.136} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.681} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.263} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.619} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.260} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.787} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.222} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.843} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.197} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.691} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.220} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.866} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.147} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.848} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
MNASNet: 0.75 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.830} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.072} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.524} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.097} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.689} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.142} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.212} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.674} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.298} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.796} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.811} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.230} \\
MNASNet: 1.0 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.588} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.213} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.698} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.235} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.220} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.527} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.185} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.839} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.236} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.433} \\
 & RP & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.521} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.072} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.830} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.064}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.125} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
MNASNet: 1.3 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.583} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.842} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.078} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.765} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.139} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.530} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.164} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.114} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.743} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.153} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.523} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.848} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.073} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.743} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.109}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.604} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.884} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.107} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.312} \\
\bottomrule
\end{tabular}
    }
\end{table}





\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the MobileNetV2 variants.}
    \label{tab:Averages_of_MobileNetV2}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
MobileNet V2 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.565} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.214} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.776} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.164} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.788} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.294} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.485} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.200} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.773} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.201} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.708} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.193} \\
 & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.199}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.927} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.140}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.889} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.296} \\
\bottomrule
\end{tabular}
    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the MobileNetV3 variants.}
    \label{tab:Averages_of_MobileNetV3}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
MobileNet V3: Large & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.507} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.758} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.146} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.765} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.178} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.561} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.227} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.777} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.185} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.195} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.683} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.272} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.838} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.191} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.818} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.318} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.792}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.234} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.908} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.109} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.910}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.135}} \\
MobileNet V3: Small & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.473} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.807} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.153} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.639} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.283} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.474} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.091} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.731} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.165} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.697} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.150} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.727} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.236} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.912}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.848} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.148} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.246} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.822} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.174} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.207} \\
\bottomrule
\end{tabular}
    }
\end{table}







\subsubsection{Extreme Nets}

Three metric tables were constructed, each corresponding to a model within the \gls{CV} family.
% Dense Net
Table \ref{tab:Averages_of_DenseNet} presents the DenseNet results, where the variants include depths of 121, 161, 169, or 201 layers. For the 169 and 201 layer variants, the \gls{PMix} method achieved superior performance, whereas the \gls{RP} method was the best for the 161 layer variant. For the 121 layer variant, the \gls{PMix} method obtained the highest Cohen kappa score, while the \gls{RP} method excelled in the F1-Score and achieved perfect precision. Overall, the DenseNet 161 with \gls{RP}, DenseNet 201 with \gls{PMix}, and DenseNet 121 with \gls{RP} achieved the best scores in terms of Cohen kappa, F1-Score, and precision metrics, respectively. Notably, the DenseNet 161 with \gls{RP} demonstrated a good balance across metrics, attaining the best Cohen kappa score and the second-best F1 and precision scores.
% SqueezeNet
Table~\ref{tab:Averages_of_SqueezeNet} exhibits the SqueezeNet results for versions 1.0 and 1.1. The optimized version 1.1 achieved the highest scores when paired with the \gls{PMix} method, attaining the best Cohen kappa and F1 scores. When combined with the \gls{RP} method, the optimized version 1.1 achieved the best precision score. Both combinations demonstrated generally strong performance across all metrics.
% VGG
Table \ref{tab:Averages_of_VGG} details the VGG scores across variants with 11, 13, 16, or 19 layers, with or without Batch Normalization (BN). The \gls{RP} and \gls{PMix} methods achieved the best scores for all variants, though some cases exhibited higher dispersion. Notably, the combination of \mbox{VGG 16} with \gls{RP} excelled in Cohen kappa and precision metrics, while \mbox{VGG 16 BN} with \gls{PMix} achieved the highest F1-Score. However, the \mbox{VGG 16} with \gls{RP} combination showed considerable dispersion in the F1-Score metric, making \mbox{VGG 16 BN} with \gls{PMix} a more reliable choice.
% Inter-Family Comparison
% the combinations \mbox{SqueezeNet 1.1} with \gls{PMix} and \mbox{VGG 16 BN} with \gls{PMix} stand out. Specifically, \mbox{SqueezeNet 1.1} with \gls{PMix} achieved the best Cohen kappa, while \mbox{VGG 16 BN} with \gls{PMix} attained the highest F1-score among all Extreme Nets \gls{CV} family models. The Figure~\ref{fig:Time_of_Extreme nets} illustrates that \mbox{SqueezeNet 1.1} with \gls{PMix} outperforms most other variants in terms of inference speed. Additionally, Table~\ref{tab:Memory_of_Extreme Models} shows that this combination also ranks as the smallest in memory consumption. Thence, we choose two methods as representative of this section:
% \begin{itemize}
%     \item VGG 16 BN with \gls{PMix};
%     \item and SqueezeNet 1.1 with \gls{PMix}.
% \end{itemize}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the DenseNet variants.}
    \label{tab:Averages_of_DenseNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
DenseNet: 121 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.621} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.248} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.795} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.190} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.196} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.225} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.099} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{1.000}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.212} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.902} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
DenseNet: 161 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.557} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.168} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.724} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.191} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.788} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.191} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.676} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.239} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.818} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.847} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.204} \\
 & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.238} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.907} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.085} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
DenseNet: 169 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.480} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.103} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.700} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.211} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.767} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.188} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.653} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.261} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.857} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.132} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.192} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.636} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.275} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.848} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.133} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.153} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.222} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.088} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.917} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
DenseNet: 201 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.600} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.256} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.291} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.558} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.223} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.058}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.701} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.351} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.683} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.183} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.773} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.179} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.889} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.175} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.140} \\
\bottomrule
\end{tabular}
    }
\end{table}


\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the SqueezeNet variants.}
    \label{tab:Averages_of_SqueezeNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
SqueezeNet: 1.0 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.591} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.231} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.804} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.192} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.586} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.194} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.742} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.197} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.812} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.217} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.787} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.206} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.816} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.194} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.958} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.271} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.838} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.210} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.856} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.211} \\
SqueezeNet: 1.1 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.819} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.080}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.700} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.105} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.450} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.112} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.794} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.161} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.646} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.904} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.181} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.914} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.168} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
\bottomrule
\end{tabular}
    }
\end{table}


\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the VGG variants.}
    \label{tab:Averages_of_VGG}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
VGG: 11 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.659} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.840} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.182} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.811} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.201} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.174} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.790} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.829} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.192} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.855} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.188} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.222} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.903} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.146} \\
VGG: 11 BN & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.562} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.848} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.073} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.764} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.132} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.545} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.151} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.849} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.169} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.921} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.098} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.926} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
VGG: 13 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.244} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.863} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.121} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.819} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.110} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
VGG: 13 BN & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.530} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.164} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.833} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.114} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.743} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.153} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.246} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.921} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.131} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.198} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.873} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.189} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
VGG: 16 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.583} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.860} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.052}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.780} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.113} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.904}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.181} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.930} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.151} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
VGG: 16 BN & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.541} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.196} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.701} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.569} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.177} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.828} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.152} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.625} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.856} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.125} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.960}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.115} \\
VGG: 19 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.117} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.855} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.052} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.109} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.479} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.078} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.776} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.139} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.736} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.172} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.688} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.217} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.879} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.077} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.840} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
VGG: 19 BN & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.549} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.790} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.152} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.757} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.172} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.553} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.262} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.764} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.211} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.743} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.215} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.927} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.708} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.885} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.123} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.195} \\
\bottomrule
\end{tabular}
    }
\end{table}






\subsubsection{Efficiency Nets}

Three score tables were constructed for the Efficiency-Oriented \gls{CV} family.
% EfficientNet
Table~\ref{tab:Averages_of_EfficientNet} lists the results for EfficientNet variants ranging from B0, the smallest, to B4, the largest, in terms of parameter scaling. The \gls{PMix} projection achieved the highest scores for variants B0, B1, and B2. For the B3 variant, the \gls{MTF} method excelled in the F1-score, while the \gls{RP} method performed better for the other metrics. Overall, the combination of \mbox{EfficientNet B1} with \gls{PMix} emerged as the top performer.
% EfficientNetV2
Table~\ref{tab:Averages_of_EfficientNetV2} presents the scores for EfficientNet V2, with the \gls{PMix} projection outperforming all other methods.
% ShuffleNet V2
Table~\ref{tab:Averages_of_ShuffleNetV2} displays the metrics for ShuffleNet V2 variants, which include multipliers of $\times 0.5$, $\times 1.0$, $\times 1.5$, and $\times 2.0$ on the number of channels in each architectural block. Across all variants, the \gls{PMix} projection method outperformed all others. Specifically, the best scores for ShuffleNet V2 were achieved with the $\times 0.5$ and $\times 1.0$ variants combined with the \gls{PMix} method.
% Inter-Family Comparison
% When analyzing the results from Tables  \ref{tab:Averages_of_EfficientNet}, \ref{tab:Averages_of_EfficientNetV2}, and \ref{tab:Averages_of_ShuffleNetV2}, it is evident that the usage of \mbox{EfficientNet V2}, \mbox{ShuffleNet V2 $\times 0.5$}, and \mbox{ShuffleNet V2 $\times 1.0$}  with \gls{PMix} achieved high scores across all metrics, including the best Cohen kappa and F1 scores, as well as the second-best precision. Among these, the \mbox{ShuffleNet V2 $\times 0.5$} with \gls{PMix} was the most efficient in terms of memory usage, as shown in Table~\ref{tab:Memory_of_Efficiency-Oriented}, while being one of the fastest in inference, as visible in the Figure \ref{fig:Time_of_Efficiency Oriented}. Thus, only the following method was selected for this section:
% \begin{itemize}
% 	\item ShuffleNet V2$\times0.5$ with \gls{PMix}.
% \end{itemize}






\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the EfficientNet variants.}
    \label{tab:Averages_of_EfficientNet}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
EfficientNet: B0 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.569} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.177} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.828} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.152} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.507} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.206} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.694} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.736} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.856} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.142} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.882} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.148} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.841} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.231} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.916} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.134} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} \\
EfficientNet: B1 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.517} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.247} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.755} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.178} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.688} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.278} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.503} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.131} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.735} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.186} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.757} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.694} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.294} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.841} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.196} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.856} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.211} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.140}} \\
EfficientNet: B2 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.436} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.161} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.168} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.546} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.344} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.473} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.117}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.671} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.228} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.217} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.682} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.276} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.858} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.179} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.806} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.192} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.926} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.093} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.140}} \\
EfficientNet: B3 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.583} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.841} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.082}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.579} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.229} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.863} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.719} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.339} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.696} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.210} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.817} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.176} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.604} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.225} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.788} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.181} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.209} \\
\bottomrule
\end{tabular}
    }
\end{table}




\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the EfficientNetV2 variants.}
    \label{tab:Averages_of_EfficientNetV2}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
EfficientNet V2 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.600} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.200} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.826} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.800} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.197} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.545} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.151}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.849} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.708} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.234} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.895} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.080}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.840} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.144} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.083} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
\bottomrule
\end{tabular}

    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the ShuffleNetV2 variants.}
    \label{tab:Averages_of_ShuffleNetV2}
    \adjustbox{max width=\columnwidth,center}{
\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
ShuffleNet V2: x0.5 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.523} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.208} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.784} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.736} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.210} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.473} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.851} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.280} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.821} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.231} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.876} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.190} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
ShuffleNet V2: x1.0 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.504} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.194} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.744} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.180} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.688} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.285} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.591} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.202} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.122} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.775} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.727} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.236} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.095} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.885} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.160} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
ShuffleNet V2: x1.5 & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.489} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.156} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.772} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.122} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.659} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.248} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.592} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.220} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.207} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.767} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.301} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.800} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.115}} \\
ShuffleNet V2: x2.0 & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.625} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.226} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.110} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.179} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.527} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.700} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.230} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.234} \\
 & RP & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.480} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.235} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.789} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.189} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.629} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.358} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.106} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.847} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.173} \\
\bottomrule
\end{tabular}

    }
\end{table}






\subsubsection{Further architectures}

Three score tables were generated for the remaining computer vision models.
% AlexNet
The Table \ref{tab:Averages_of_AlexNet} contains the scores of AlexNet. Notably, the \gls{PMix} projection earned the best scores for all metrics in that table.
% ConvNeXt
Table~\ref{tab:Averages_of_ConvNeXt} presents the results for the ConvNeXt model, with variants including Tiny, Small, Base, and Large in terms of parameter size. Among these variants, the \gls{RP} and \gls{PMix} methods achieved the best scores overall, though some instances, such as \gls{RP} with the Tiny variant, exhibited higher dispersion, particularly for the Cohen kappa score. Specifically, the \gls{PMix} method with the \mbox{ConvNeXt Tiny} variant achieved the highest F1-score, while the \gls{PMix} method with the \mbox{ConvNeXt Small} variant obtained the best Cohen kappa and precision scores, and also secured the second best F1-score.
% RegNet 
Table~\ref{tab:Averages_of_RegNet} exhibits the results for RegNet variants, categorized into RegNetX and RegNetY design spaces~\cite{RegNet}, with varying float operations per second rates such as 400 Mega Flops (MF) or 16 Giga Flops (GF). Among these variants, several achieved scores above the third quartile across all metrics. For the X space, notable cases include \gls{RP} with the 400 MF and 800 MF variants, and \gls{PMix} with the 800 MF, 3.2 GF, 8 GF, and 16 GF variants. For the Y space, noteworthy cases are \gls{RP} with the 400 MF, 1.6 GF, 16 GF, and 32 GF variants, and \gls{PMix} with the 800 MF, 3.2 GF, and 16 GF variants. Among these high-scoring variants, the \gls{PMix} method with the \mbox{RegNet X 3.2 GF}, \mbox{RegNet X 800 MF}, \mbox{RegNet Y 400 MF}, and \mbox{RegNet Y 800 MF} variants achieved the best Cohen kappa and F1 scores, and the third best precision score. 
%Of these top-performing combinations, the \mbox{RegNet Y 400 MF} with \gls{PMix} had the lowest memory usage, as shown in Table~\ref{tab:Memory_of_Diverse}, while the \mbox{RegNet X 800 MF} with \gls{PMix} had the fastest inferences of the model variants, as seen in Figure \ref{fig:Time_of_Diverse}.
% Inter-Family Comparison
When evaluating the top-performing models from each type within the Diverse \gls{CV} family, the \mbox{RegNet Y 400 MF} with \gls{RP}, and the \mbox{RegNet X 800 MF} and AlexNet with \gls{PMix} achieved the highest scores.
%Notably, the \mbox{RegNet Y 400 MF} with \gls{RP} exhibited the lowest memory usage, as shown in Table~\ref{tab:Memory_of_Diverse}, while AlexNet had the fastest inference times across all projections, as illustrated in Figure~\ref{fig:Time_of_Diverse}. Hence, two methods were chosen for this section:
% \begin{itemize}
%     \item AlexNet with \gls{PMix};
%     \item and RegNet Y 400 MF with \gls{RP}.
% \end{itemize}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the AlexNet variants.}
    \label{tab:Averages_of_AlexNet}
    \adjustbox{max width=\columnwidth,center}{

\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
AlexNet & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.545} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.151}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.849} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.598} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.247} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.827} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.174} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.773} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.704} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.204} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.819} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.168} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.135} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083}} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
\bottomrule
\end{tabular}

    }
\end{table}





\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the ConvNeXt variants.}
    \label{tab:Averages_of_ConvNeXt}
    \adjustbox{max width=\columnwidth,center}{

\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
ConvNeXt: Base & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.536} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.157} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.137} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.764} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.170} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.473} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.144} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.789} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.159} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.667} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.268} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.883} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.926} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.093} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
ConvNeXt: Large & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.611} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.239} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.845} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.124} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.785} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.183} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.545} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.151} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.849} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
ConvNeXt: Small & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.708} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.885} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.123} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.833} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.195} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.611} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.239} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.845} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.124} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.785} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.183} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.926} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.093} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.093} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130}} \\
ConvNeXt: Tiny & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.688} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.241} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.884} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.826} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.168} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.523} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.833} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.903} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.113} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.157} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.939} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.135} \\
\bottomrule
\end{tabular}

    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the RegNet variants.}
    \label{tab:Averages_of_RegNet}
    \adjustbox{max width=\columnwidth,center}{

\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
RegNet: X; 16 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.668} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.226} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.841} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.202} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.542} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.226} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.771} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.736} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.303} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.838} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.142} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.902} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.178} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
RegNet: X; 1.6 GF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.515} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.174} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.817} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.123} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.736} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.553} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.176} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.829} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.114} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.764} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.170} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.768} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.233} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.865} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.195} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.101} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.918} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.172} \\
RegNet: X; 32 GF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.508} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.095} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.830} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.094} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.735} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.117} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.527} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.185} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.812} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.157} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.705} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.292} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.930} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
RegNet: X; 3.2 GF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.461} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.095} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.810} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.088} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.657} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.278} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.550} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.098} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.772} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.122} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.800} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.197} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.914} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.168} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.931} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
RegNet: X; 400 MF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.523} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.831} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.104} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.778} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.150} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.479} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.113} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.773} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.095} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.550} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.098} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.791} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.119} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} \\
RegNet: X; 800 MF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.594} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.278} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.857} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.145} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.720} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.306} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.402} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.117} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.656} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.238} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.667} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.173} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.115} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
RegNet: X; 8 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.545} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.151} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.849} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.151} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.530} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.164} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.812} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.157} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.742} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.169} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.198} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.095} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.970}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.101}} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.939} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.135} \\
RegNet: Y; 16 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.612} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.196} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.811} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.149} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.818} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.197} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.477} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.118} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.778} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.117} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.727} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.167} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
RegNet: Y; 1.6 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.636} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.259} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.846} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.217} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.500} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.000}} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.837} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.090} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.129} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.795} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.218} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.914} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.092} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
RegNet: Y; 32 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.583} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.222} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.822} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.158} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.764} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.170} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.568} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.226} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.823} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.185} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.667} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.222} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.883} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.074} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.819} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.137} \\
RegNet: Y; 3.2 GF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.712} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.280} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.881} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.143} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.826} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.199} \\
 & MTF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.547} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.246} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.753} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.206} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.758} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.212} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.826} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.234} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.119} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
 & \purpole{PMix} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
RegNet: Y; 400 MF & GAF & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.590} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.260} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.823} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.173} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.211} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.475} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.112} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.690} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.193} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.198} \\
 & RP & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.773} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.236} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.919} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.861} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
RegNet: Y; 800 MF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.521} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.072} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.832} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.061}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.742} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.121} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.486} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.191} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.660} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.240} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.713} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.196} \\
 & RP & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.257} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.902} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.121} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.296} \\
 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
RegNet: Y; 8 GF & GAF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.508} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.029} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.790} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.123} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.750} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.158} \\
 & MTF & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.482} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.166} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.738} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.158} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.648} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.303} \\
 & RP & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.927} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
 & \purpole{PMix} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.908} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.109} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.868} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.176} \\
\bottomrule
\end{tabular}

    }
\end{table}






\subsection{Non-computer vision models comparison}


Table~\ref{tab:Averages_of_Non-CV} presents the scores for temporal series models that are not based on \gls{CV} techniques. These models include Individual \mbox{Ordinal TDE}, Random Interval Classifier, \gls{RISEC}, \mbox{TS Fresh} Classifier, \gls{TDE}, and \mbox{WEASEL V2} achieved high scores across all metrics. Among these, \gls{RISEC} and \gls{TDE} surpassed the other models in every score metric. 
Specifically, \gls{RISEC} demonstrated faster inference while \gls{TDE} had lower memory consumption. So, we elect the two methods below as the best models of this section.

%Specifically, \gls{RISEC} demonstrated faster inference, as shown in Figure \ref{fig:Time_of_Non CV},while \gls{TDE} had lower memory consumption, according to Table~\ref{tab:Memory_of_Non-CV}. So, we elect the two methods below as the best models of this section:
% \begin{itemize}
%     \item \gls{RISEC};
%     \item and \gls{TDE}.
% \end{itemize}





\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the Non-CV variants.}
    \label{tab:Averages_of_Non-CV}
    \adjustbox{max width=\columnwidth,center}{

\begin{tabular}{llll}
\toprule
Model & Cohen Kappa & F1 Score & Precision \\
\midrule
Arsenal & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.639} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.252} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.804} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.140} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.819} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.204} \\
BOSS Ensemble & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.688} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.241} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.884} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.826} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.168} \\
Zhao's CNN Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.939} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.135} \\
Interval Forest Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
Catch 22 Classifier & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
Continuous Interval Tree & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.632} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.240} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.856} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.112} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.165} \\
Contractable BOSS & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.792} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.234} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.919} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.087} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.882} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.148} \\
DrCIF Classifier & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.904} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.181} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.930} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.151} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
Elastic Ensemble & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.812} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.188} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.893} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.097} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
Wang's FCN Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.842} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.210} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.901} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.152} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
Inception Time Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.799} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.242} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.898} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.155} \\
Individual BOSS & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.169} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.921} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.098} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
Individual Inception Classifier & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.694} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.228} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.858} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} \\
Individual Ordinal TDE & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
Individual TDE & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
K-Neighbors Time Series Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.927} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.116} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.931} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.166} \\
LITE Time Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.771} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.249} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.908} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.109} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.868} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.176} \\
Wang's MLP Classifier & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.485} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.050}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.821} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.102} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.722} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
MUSE & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
Ordinal TDE & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
RDST Classifier & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.633} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.196} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.842} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.125} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.819} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.137} \\
REDCOMETS & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.508} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.095} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.817} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.101} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.743} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.153} \\
Random Interval Classifier & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
RISEC & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.971}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.068} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
Rocket Classifier & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.859} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.122} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.868} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.176} \\
Rotation Forest Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.854} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.932} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.111} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.910} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.172} \\
Shape DTW & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.729} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.225} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.106} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.868} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.176} \\
Shapelet Transform Classifier & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.625} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.873} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.067}} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.803} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.131} \\
Summary Classifier & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.883} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.913} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
Supervised Time Series Forest & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.862} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.184} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.154} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
TS Fresh Classifier & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
TDE & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.971}} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.068} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
Time Series Forest Classifier & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.896} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.167} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.938} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.093} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
WEASEL & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.875} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.199} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.943} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.086} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.924} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.140} \\
WEASEL V2 & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.917} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.163} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.083} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.944} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.130} \\
\bottomrule
\end{tabular}

    }
\end{table}



\subsection{Comparison of the top-performing models}


The best combinations between models and projections from previous analyses are aggregated in Table~\ref{tab:Averages_of_best models}. The top-performing models include the \gls{TDE}, the \gls{RISEC}, and the \mbox{Wide ResNet 100-2} with \gls{PMix}. While the \mbox{Wide ResNet} achieved the highest Cohen kappa score, it is notable that this model was the second largest in terms of memory usage, as indicated in Table~\ref{tab:Memory_of_best models}. Additionally, it did not achieve the highest F1-Score or precision and was the second slowest in terms of inference speed, as shown in Figure~\ref{fig:Time_of_Best Models}. In contrast, the \gls{TDE} and \gls{RISEC} models excelled in usability and security metrics, including F1-Score and precision. They also demonstrated superior performance in terms of inference speed and memory efficiency. Consequently, while the \gls{CV} approach, represented by the \mbox{Wide ResNet 100-2} with \gls{PMix}, achieved higher accuracy, the non-\gls{CV} approach, embodied by the \gls{TDE} and \gls{RISEC}, offers better resource efficiency and speed, making it a more practical choice for applications requiring lower resource consumption and faster performance.




\begin{table}[t!]
    \centering
    \caption{Averages and standard deviations of the folds evaluation for the best models.}
    \label{tab:Averages_of_best models}
    \adjustbox{max width=\columnwidth,center}{

\begin{tabular}{lllll}
\toprule
Model & Projection & Cohen Kappa & F1 Score & Precision \\
\midrule
AlexNet & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
MNASNet: 1.0 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
RISEC & Not projected & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.971}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.068}} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
RegNet: Y; 400 MF & RP & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
ResNet: 50 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
ShuffleNet V2: x0.5 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
SqueezeNet: 1.1 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
SwinTV2: S & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
TDE & Not projected & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.938} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.155} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.971}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.068}} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.972}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.096}} \\
VGG: 16 BN & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.896} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.198} & \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.960} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.075} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.951} $\pm$ \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.115} \\
ViT: B 32 & \purpole{PMix} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.917} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.163} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.955} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.083} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
WiResNet: 101-2 & \purpole{PMix} & \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.955}} $\pm$ \textbf{\textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.101}} & \textcolor[rgb]{0.0000000000,0.7000000000,0.0}{0.967} $\pm$ \textcolor[rgb]{0.3000000000,0.3000000000,0.5}{0.078} & \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.944} $\pm$ \textcolor[rgb]{1.0000000000,0.2000000000,0.2}{0.130} \\
\bottomrule
\end{tabular}


    }
\end{table}



\begin{table}[t!]
    \centering
    \caption{Memory size (in MB) of the top-12 best-performing models.}

    \adjustbox{max width=\columnwidth,center}{
    \begin{tabular}{lr}
        \toprule
        Neural Network & \begin{tabular}{c} Size (MB)\end{tabular} \\
        \midrule
        TDE & 0.586264 \\
        RISEC & 1.171312 \\
        ShuffleNet V2: x0.5 & 1.376760 \\
        SqueezeNet: 1.1 & 2.894136 \\
        MNASNet: 1.0 & 12.420792 \\
        RegNet: Y; 400 MF & 15.617376 \\
        \bottomrule
    \end{tabular}
    \begin{tabular}{lr}
        \toprule
        Neural Network & \begin{tabular}{c} Size (MB)\end{tabular} \\
        \midrule
        ResNet: 50 & 94.049840 \\
        SwinTV2: S & 195.880352 \\
        AlexNet & 228.048184 \\
        ViT: B 32 & 349.827128 \\
        WiResNet: 101-2 & 499.369720 \\
        VGG: 16 BN & 537.109104 \\
        \bottomrule
    \end{tabular}
    }

    \label{tab:Memory_of_best models}
\end{table}



\begin{figure}[ht!]
    \includegraphics[width=\columnwidth, trim={17em 0 0 0}, clip]{benchmark_of_best models.pdf}
    \caption{Inference time (in ms) of best achieved models.}
    \label{fig:Time_of_Best Models}
\end{figure}










\section{Discussion and Limitations}
\label{sec:Limitations}

Despite the promising results, some limitations can be considered. The experiments utilized only the BUTPPG dataset for testing. This has a series of implications in our context. Firstly, even though the proposed method allowed the use of \gls{CV} models with a good performance in the BUTPPG dataset, the same could not necessarily be concluded for different datasets. This is because different methods of measurement, sensor qualities, signal lengths, and individual medical conditions could lead to alterations on the obtained performance. One evidence of that is the absence of confirmed \gls{CA} cases in the \gls{BUTPPG} dataset, which could be present in external data. Furthermore, the small size of the dataset resulted in a reduced testing dataset, which makes the obtained results less general, that is, unreliable when we consider the possible variability of data that is external to the dataset. A small dataset size also implies that the deep learning models had less data samples to effectively learn. This contrasts with the usual treatment for deep learning, where usually large amounts of data feed the training of the model, allowing the proper adjustment of the large set of parameters. Therefore, our experiments would be more complete if our experiments tested on different and larger datasets.

Another constraint is that the experiments did not explore all available options in terms of models. For instance, our experiments left out some of the models of the Pytorch and the Aeon libraries. Examples of them are the GoogLeNet~\cite{GoogLeNet}, from the Pytorch library, and the Hydra Classifier~\cite{HydraClassifier}, from the Aeon library.  Additionally, models external to these libraries, such as Xception~\cite{Xception} available in the Keras library~\cite{Keras}, were not tested. Furthermore, not all variants of the tested models were evaluated, such as EfficientNet~B7. Additionally, the hyperparameters of the projection methods, such as the number of dimensions in \gls{RP}, were not optimized. Exploring a broader range of options, including different libraries and hyperparameter search techniques like Optuna, could yield more comprehensive results.

Finally, additional limitations were identified at the implementation level.  Firstly, the implementation utilized random oversampling to balance the dataset. However, alternative methods specifically designed for time series data, such as those described in~\cite{TimeSeriesAugmentation}, could have been employed.  These methods not only balance the dataset but also can augment it. Secondly, resizing transforms were used to adapt projection images to model inputs, potentially leading to significant loss of information for matrix images that encode pixel relationships. As a consequence, the \gls{CV} models not performed as good as they could.
While early stopping was implemented to prevent overfitting, future work should involve sensitivity analysis to determine if this mechanism impacted the convergence of high-complexity models.
Lastly, benchmarking metrics were measured using the Python standard API, which may be limited by the interpreter. Additionally, our measurements did not control the environment where measured the inference time. This could imply that competed with the experimental measurements. Therefore, improvements at the implementation level could include applying time series augmentation techniques, resizing images without distortion by using integer multipliers and padding, researching and optimizing early stopping methods, and conducting measurements in a more controlled environment using low-level interfaces.






\section{Conclusions}
\label{sec:Limitations}


This work, presented a study on \gls{SQA} for \gls{PPG} signals, mainly following an 1D-to-2D-projection approach in combination with \gls{CV} techniques. The investigated projection-based approach involved transforming 1D signals onto 2D images using the \gls{RP}, \gls{GAF}, and \gls{MTF} methods. In addition to these methods, we proposed a mixed approach combining them. The results indicate that the \gls{RP} and \gls{PMix} projection methods outperformed the \gls{GAF} and \gls{MTF} methods, with \gls{RP} and \gls{PMix} yielding similar outcomes. Although the set of machine learning models was extensive, the \gls{BUTPPG} dataset was small and unbalanced, limiting the conclusiveness of the results. Consequently, the experiment should be replicated on a larger dataset, either by using data augmentation techniques to balance and expand the BUTPPG dataset or by utilizing a different dataset with more samples.

The experiments provided insights of the importance of the proposed \gls{PMix} approach. One key finding is that the \gls{PMix} method appeared more frequently among the best-performing results compared to the other projection methods. Specifically, as depicted in Table~\ref{tab:Averages_of_best models}, all but one top-performing approaches used the \gls{PMix} method.
These results demonstrate that the proposed \gls{PMix} fusion strategy effectively mitigates the limitations of individual projection techniques by providing a more comprehensive spatial encoding of the \gls{PPG} morphology.
Our experiments also demonstrate the overall effectiveness of projection methods. Notably, in the best-performing models listed in Table \ref{tab:Averages_of_best models}, the \gls{PMix} method combined with Wide ResNet outperformed the baseline time series classification models in terms of the Cohen Kappa score.
This indicates that a projection-based approach can be highly accurate for both binary \gls{SQI} classes.
While this highlights the viability of the projection-based approach alongside other time series classifiers, the conventional time series classifiers achieved higher F1 and Precision scores, suggesting they performed better for the positive \gls{SQI} class. An additional drawback is that combining projection-based methods with \gls{CV} models is computationally more expensive and requires more memory than using 1D classifiers. Nevertheless, the projection-based approach proved to be effective for \gls{SQA}, fulfilling our original objective described in Section~\ref{sec:problem}.


% Aplicabilidade dos métodos de projeção
The results reveal that the proposed method is a promising tool for real-life applications, as presented in Section~\ref{sec_intro}.
% - Ferramenta de engenharia de IA
This technique allows \gls{AI} engineers to trade memory and computational cost for improved accuracy. By combining multiple projection methods, the approach increases image size and incurs 1D-to-2D conversion costs for each projection.
% - Aplicação em dispositivos vestíveis
For that reason, even though the \gls{CV} models incorporated into the proposed method have sufficiently low latency to support a responsive application, they may not be advisable for wearable devices, such as smartwatches, due to memory constraints.
% - Aplicação em monitoramento médico remoto
It is necessary to process the signal on a remote device with greater memory capacity, such as a server in a remote healthcare environment.
% - Conclusão
Therefore, this method is suitable for remote healthcare applications.


Several opportunities for improvement exist, some of which are discussed in Section~\ref{sec:Limitations}. For instance, this study utilized the only currently available public dataset, which is constrained by its size, variety, and recording length. Future work should involve the development of novel datasets to enable more extensive experiments and cross-dataset validation.
%Additionally, our experiments did not exhaustively explore all 1D and 2D open-source models, nor did they systematically vary the parameters of the projections and \gls{ML} architectures. Investigating these variables could reveal critical relationships between model selection and projection performance.
Beyond expanding the model selection, it is essential to benchmark the proposed method against specialized \gls{SQA} techniques. This would provide a more robust assessment of the method's comparative performance. Finally, the implementation could be refined through optimized pre-processing techniques, advanced training strategies, and a more robust experimental setup.


\section*{Declaration of AI-assisted technologies}

During the preparation of this work the authors used ChatGPT\texttrademark and Grammarly\texttrademark to review English usage and grammatical correction. After using these tools, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.


\bibliographystyle{elsarticle-num}
\bibliography{bibliografia}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
